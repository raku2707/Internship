{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccc1960f",
   "metadata": {},
   "source": [
    "# ASSIGNMENT-4\n",
    "\n",
    "## WEB SCRAPING\n",
    "\n",
    "\n",
    "1. **[Scrape the details of most viewed videos on YouTube from Wikipedia.](#videos)**\n",
    "2. **[Scrape the details teamIndia’s international fixtures from bcci.tv.](#BCCI)**\n",
    "3. **[Scrape the details of State-wise GDP ofIndia fromstatisticstime.com.](#statistics)**\n",
    "4. **[Scrape the details of trending repositories on Github.com.](#Github)**\n",
    "5. **[Scrape the details of top 100 songs on billiboard.com.](#billiboard)**\n",
    "6. **[Scrape the details of Highest sellingnovels.](#novels)**\n",
    "7. **[Scrape the details most watched tv series of all time from imdb.com.](#imdb)**\n",
    "8. **[Details of Datasetsfrom UCI machine learning repositories.](#repositories)**\n",
    "9. **[Scrape the details of Data science recruiters.](#naukri)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3f09d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\hp\\anaconda3\\lib\\site-packages (4.9.0)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (0.10.2)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from selenium) (1.26.9)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.1.1)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.3)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\hp\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "#Insall selenium\n",
    "\n",
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bf12cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import some necessary libraries\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from selenium.common.exceptions import StaleElementReferenceException, NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f374eae",
   "metadata": {},
   "source": [
    "<a id=\"videos\"> </a>\n",
    "## 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url = https://en.wikipedia.org/wiki/List_of_most_viewed_YouTube_videos\n",
    "\n",
    "You need to find following details:\n",
    "\n",
    "    A) Rank\n",
    "    B) Name\n",
    "    C) Artist\n",
    "    D) Upload date\n",
    "    E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4af32bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1c8265b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the youtube page on automated chrome browser\n",
    "\n",
    "driver.get('https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69612fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating page for top videos by xpath\n",
    "\n",
    "option = driver.find_element(By.XPATH,'/html/body/div[2]/div/div[3]/main/header/nav/div/input')\n",
    "option.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e8c226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locating page for top youtube videos by xpath\n",
    "\n",
    "topvideos = driver.find_element(By.XPATH,'/html/body/div[2]/div/div[3]/main/header/nav/div/div/div/div/ul/li[2]/a/div') \n",
    "topvideos.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "accb4dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.', '11.', '12.', '13.', '14.', '15.', '16.', '17.', '18.', '19.', '20.', '21.', '22.', '23.', '24.', '25.', '26.', '27.', '28.', '29.', '30.']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Rank\n",
    "\n",
    "Rank=[]                 \n",
    "\n",
    "rank=driver.find_elements(By.XPATH,'//tr/td[@align=\"center\"][1]')\n",
    "for i in rank[0:30]:\n",
    "    title=i.text\n",
    "    Rank.append(title)\n",
    "\n",
    "print(len(Rank),Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "106d46ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['\"Baby Shark Dance\"[4]', '\"Despacito\"[7]', '\"Johny Johny Yes Papa\"[14]', '\"Bath Song\"[15]', '\"Shape of You\"[16]', '\"See You Again\"[18]', '\"Phonics Song with Two Words\"[23]', '\"Wheels on the Bus\"[24]', '\"Uptown Funk\"[25]', '\"Learning Colors – Colorful Eggs on a Farm\"[26]', '\"Gangnam Style\"[27]', '\"Masha and the Bear – Recipe for Disaster\"[32]', '\"Dame Tu Cosita\"[33]', '\"Axel F\"[34]', '\"Sugar\"[35]', '\"Roar\"[36]', '\"Counting Stars\"[37]', '\"Sorry\"[38]', '\"Baa Baa Black Sheep\"[39]', '\"Thinking Out Loud\"[40]', '\"Waka Waka (This Time for Africa)\"[41]', '\"Dark Horse\"[42]', '\"Lakdi Ki Kathi\"[43]', '\"Faded\"[44]', '\"Perfect\"[45]', '\"Let Her Go\"[46]', '\"Girls Like You\"[47]', '\"Humpty the train on a fruits ride\"[48]', '\"Lean On\"[49]', '\"Bailando\"[50]']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the video name\n",
    "\n",
    "Name=[]                 \n",
    "\n",
    "video=driver.find_elements(By.XPATH,'//*[@id=\"mw-content-text\"]/div/table[2]/tbody/tr/td[2]')\n",
    "for i in video[0:30]:\n",
    "    title=i.text\n",
    "    Name.append(title)\n",
    "\n",
    "print(len(Name),Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41481a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 [\"Pinkfong Baby Shark - Kids' Songs & Stories\", 'Luis Fonsi', 'LooLoo Kids', 'Cocomelon – Nursery Rhymes', 'Ed Sheeran', 'Wiz Khalifa', 'ChuChu TV', 'Cocomelon – Nursery Rhymes', 'Mark Ronson', 'Miroshka TV', 'Psy', 'Get Movies', 'El Chombo', 'Crazy Frog', 'Maroon 5', 'Katy Perry', 'OneRepublic', 'Justin Bieber', 'Cocomelon – Nursery Rhymes', 'Ed Sheeran', 'Shakira', 'Katy Perry', 'Jingle Toons', 'Alan Walker', 'Ed Sheeran', 'Passenger', 'Maroon 5', 'Kiddiestv Hindi – Nursery Rhymes & Kids Songs', 'Major Lazer', 'Enrique Iglesias']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Artist name\n",
    "\n",
    "Artist=[]                 \n",
    "\n",
    "artist=driver.find_elements(By.XPATH,'//*[@id=\"mw-content-text\"]/div/table[2]/tbody/tr/td[3]')\n",
    "for i in artist:\n",
    "    title=i.text\n",
    "    Artist.append(title)\n",
    "\n",
    "print(len(Artist),Artist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "779b5a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['June 17, 2016', 'January 12, 2017', 'October 8, 2016', 'May 2, 2018', 'January 30, 2017', 'April 6, 2015', 'March 6, 2014', 'May 24, 2018', 'November 19, 2014', 'February 27, 2018', 'July 15, 2012', 'January 31, 2012', 'April 5, 2018', 'June 16, 2009', 'January 14, 2015', 'September 5, 2013', 'May 31, 2013', 'October 22, 2015', 'June 25, 2018', 'October 7, 2014', 'June 4, 2010', 'February 20, 2014', 'June 14, 2018', 'December 3, 2015', 'November 9, 2017', 'July 25, 2012', 'May 31, 2018', 'January 26, 2018', 'March 22, 2015', 'April 11, 2014']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the upload date of video\n",
    "\n",
    "Upload_date=[]                 \n",
    "\n",
    "upload_date=driver.find_elements(By.XPATH,'//*[@id=\"mw-content-text\"]/div/table[2]/tbody/tr/td[5]')\n",
    "for i in upload_date:\n",
    "    title=i.text\n",
    "    Upload_date.append(title)\n",
    "\n",
    "print(len(Upload_date),Upload_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e611aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 ['12.85', '8.16', '6.70', '6.20', '6.00', '5.89', '5.30', '5.24', '4.92', '4.89', '4.80', '4.55', '4.35', '3.91', '3.87', '3.80', '3.79', '3.66', '3.64', '3.60', '3.59', '3.52', '3.48', '3.45', '3.45', '3.44', '3.42', '3.41', '3.38', '3.38']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Views on video\n",
    "\n",
    "Views=[]                 \n",
    "\n",
    "view=driver.find_elements(By.XPATH,'//*[@id=\"mw-content-text\"]/div/table[2]/tbody/tr/td[4]')\n",
    "for i in view:\n",
    "    title=i.text\n",
    "    Views.append(title)\n",
    "\n",
    "print(len(Views),Views)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe33b829",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Upload date</th>\n",
       "      <th>Views in Billions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>\"Baby Shark Dance\"[4]</td>\n",
       "      <td>Pinkfong Baby Shark - Kids' Songs &amp; Stories</td>\n",
       "      <td>June 17, 2016</td>\n",
       "      <td>12.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>\"Despacito\"[7]</td>\n",
       "      <td>Luis Fonsi</td>\n",
       "      <td>January 12, 2017</td>\n",
       "      <td>8.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>\"Johny Johny Yes Papa\"[14]</td>\n",
       "      <td>LooLoo Kids</td>\n",
       "      <td>October 8, 2016</td>\n",
       "      <td>6.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>\"Bath Song\"[15]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 2, 2018</td>\n",
       "      <td>6.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>\"Shape of You\"[16]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>January 30, 2017</td>\n",
       "      <td>6.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.</td>\n",
       "      <td>\"See You Again\"[18]</td>\n",
       "      <td>Wiz Khalifa</td>\n",
       "      <td>April 6, 2015</td>\n",
       "      <td>5.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.</td>\n",
       "      <td>\"Phonics Song with Two Words\"[23]</td>\n",
       "      <td>ChuChu TV</td>\n",
       "      <td>March 6, 2014</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.</td>\n",
       "      <td>\"Wheels on the Bus\"[24]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>May 24, 2018</td>\n",
       "      <td>5.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.</td>\n",
       "      <td>\"Uptown Funk\"[25]</td>\n",
       "      <td>Mark Ronson</td>\n",
       "      <td>November 19, 2014</td>\n",
       "      <td>4.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.</td>\n",
       "      <td>\"Learning Colors – Colorful Eggs on a Farm\"[26]</td>\n",
       "      <td>Miroshka TV</td>\n",
       "      <td>February 27, 2018</td>\n",
       "      <td>4.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.</td>\n",
       "      <td>\"Gangnam Style\"[27]</td>\n",
       "      <td>Psy</td>\n",
       "      <td>July 15, 2012</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12.</td>\n",
       "      <td>\"Masha and the Bear – Recipe for Disaster\"[32]</td>\n",
       "      <td>Get Movies</td>\n",
       "      <td>January 31, 2012</td>\n",
       "      <td>4.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13.</td>\n",
       "      <td>\"Dame Tu Cosita\"[33]</td>\n",
       "      <td>El Chombo</td>\n",
       "      <td>April 5, 2018</td>\n",
       "      <td>4.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.</td>\n",
       "      <td>\"Axel F\"[34]</td>\n",
       "      <td>Crazy Frog</td>\n",
       "      <td>June 16, 2009</td>\n",
       "      <td>3.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15.</td>\n",
       "      <td>\"Sugar\"[35]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>January 14, 2015</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16.</td>\n",
       "      <td>\"Roar\"[36]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>September 5, 2013</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17.</td>\n",
       "      <td>\"Counting Stars\"[37]</td>\n",
       "      <td>OneRepublic</td>\n",
       "      <td>May 31, 2013</td>\n",
       "      <td>3.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18.</td>\n",
       "      <td>\"Sorry\"[38]</td>\n",
       "      <td>Justin Bieber</td>\n",
       "      <td>October 22, 2015</td>\n",
       "      <td>3.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19.</td>\n",
       "      <td>\"Baa Baa Black Sheep\"[39]</td>\n",
       "      <td>Cocomelon – Nursery Rhymes</td>\n",
       "      <td>June 25, 2018</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20.</td>\n",
       "      <td>\"Thinking Out Loud\"[40]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>October 7, 2014</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21.</td>\n",
       "      <td>\"Waka Waka (This Time for Africa)\"[41]</td>\n",
       "      <td>Shakira</td>\n",
       "      <td>June 4, 2010</td>\n",
       "      <td>3.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22.</td>\n",
       "      <td>\"Dark Horse\"[42]</td>\n",
       "      <td>Katy Perry</td>\n",
       "      <td>February 20, 2014</td>\n",
       "      <td>3.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23.</td>\n",
       "      <td>\"Lakdi Ki Kathi\"[43]</td>\n",
       "      <td>Jingle Toons</td>\n",
       "      <td>June 14, 2018</td>\n",
       "      <td>3.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24.</td>\n",
       "      <td>\"Faded\"[44]</td>\n",
       "      <td>Alan Walker</td>\n",
       "      <td>December 3, 2015</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25.</td>\n",
       "      <td>\"Perfect\"[45]</td>\n",
       "      <td>Ed Sheeran</td>\n",
       "      <td>November 9, 2017</td>\n",
       "      <td>3.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26.</td>\n",
       "      <td>\"Let Her Go\"[46]</td>\n",
       "      <td>Passenger</td>\n",
       "      <td>July 25, 2012</td>\n",
       "      <td>3.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27.</td>\n",
       "      <td>\"Girls Like You\"[47]</td>\n",
       "      <td>Maroon 5</td>\n",
       "      <td>May 31, 2018</td>\n",
       "      <td>3.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28.</td>\n",
       "      <td>\"Humpty the train on a fruits ride\"[48]</td>\n",
       "      <td>Kiddiestv Hindi – Nursery Rhymes &amp; Kids Songs</td>\n",
       "      <td>January 26, 2018</td>\n",
       "      <td>3.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29.</td>\n",
       "      <td>\"Lean On\"[49]</td>\n",
       "      <td>Major Lazer</td>\n",
       "      <td>March 22, 2015</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30.</td>\n",
       "      <td>\"Bailando\"[50]</td>\n",
       "      <td>Enrique Iglesias</td>\n",
       "      <td>April 11, 2014</td>\n",
       "      <td>3.38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                                             Name  \\\n",
       "0    1.                            \"Baby Shark Dance\"[4]   \n",
       "1    2.                                   \"Despacito\"[7]   \n",
       "2    3.                       \"Johny Johny Yes Papa\"[14]   \n",
       "3    4.                                  \"Bath Song\"[15]   \n",
       "4    5.                               \"Shape of You\"[16]   \n",
       "5    6.                              \"See You Again\"[18]   \n",
       "6    7.                \"Phonics Song with Two Words\"[23]   \n",
       "7    8.                          \"Wheels on the Bus\"[24]   \n",
       "8    9.                                \"Uptown Funk\"[25]   \n",
       "9   10.  \"Learning Colors – Colorful Eggs on a Farm\"[26]   \n",
       "10  11.                              \"Gangnam Style\"[27]   \n",
       "11  12.   \"Masha and the Bear – Recipe for Disaster\"[32]   \n",
       "12  13.                             \"Dame Tu Cosita\"[33]   \n",
       "13  14.                                     \"Axel F\"[34]   \n",
       "14  15.                                      \"Sugar\"[35]   \n",
       "15  16.                                       \"Roar\"[36]   \n",
       "16  17.                             \"Counting Stars\"[37]   \n",
       "17  18.                                      \"Sorry\"[38]   \n",
       "18  19.                        \"Baa Baa Black Sheep\"[39]   \n",
       "19  20.                          \"Thinking Out Loud\"[40]   \n",
       "20  21.           \"Waka Waka (This Time for Africa)\"[41]   \n",
       "21  22.                                 \"Dark Horse\"[42]   \n",
       "22  23.                             \"Lakdi Ki Kathi\"[43]   \n",
       "23  24.                                      \"Faded\"[44]   \n",
       "24  25.                                    \"Perfect\"[45]   \n",
       "25  26.                                 \"Let Her Go\"[46]   \n",
       "26  27.                             \"Girls Like You\"[47]   \n",
       "27  28.          \"Humpty the train on a fruits ride\"[48]   \n",
       "28  29.                                    \"Lean On\"[49]   \n",
       "29  30.                                   \"Bailando\"[50]   \n",
       "\n",
       "                                           Artist        Upload date  \\\n",
       "0     Pinkfong Baby Shark - Kids' Songs & Stories      June 17, 2016   \n",
       "1                                      Luis Fonsi   January 12, 2017   \n",
       "2                                     LooLoo Kids    October 8, 2016   \n",
       "3                      Cocomelon – Nursery Rhymes        May 2, 2018   \n",
       "4                                      Ed Sheeran   January 30, 2017   \n",
       "5                                     Wiz Khalifa      April 6, 2015   \n",
       "6                                       ChuChu TV      March 6, 2014   \n",
       "7                      Cocomelon – Nursery Rhymes       May 24, 2018   \n",
       "8                                     Mark Ronson  November 19, 2014   \n",
       "9                                     Miroshka TV  February 27, 2018   \n",
       "10                                            Psy      July 15, 2012   \n",
       "11                                     Get Movies   January 31, 2012   \n",
       "12                                      El Chombo      April 5, 2018   \n",
       "13                                     Crazy Frog      June 16, 2009   \n",
       "14                                       Maroon 5   January 14, 2015   \n",
       "15                                     Katy Perry  September 5, 2013   \n",
       "16                                    OneRepublic       May 31, 2013   \n",
       "17                                  Justin Bieber   October 22, 2015   \n",
       "18                     Cocomelon – Nursery Rhymes      June 25, 2018   \n",
       "19                                     Ed Sheeran    October 7, 2014   \n",
       "20                                        Shakira       June 4, 2010   \n",
       "21                                     Katy Perry  February 20, 2014   \n",
       "22                                   Jingle Toons      June 14, 2018   \n",
       "23                                    Alan Walker   December 3, 2015   \n",
       "24                                     Ed Sheeran   November 9, 2017   \n",
       "25                                      Passenger      July 25, 2012   \n",
       "26                                       Maroon 5       May 31, 2018   \n",
       "27  Kiddiestv Hindi – Nursery Rhymes & Kids Songs   January 26, 2018   \n",
       "28                                    Major Lazer     March 22, 2015   \n",
       "29                               Enrique Iglesias     April 11, 2014   \n",
       "\n",
       "   Views in Billions  \n",
       "0              12.85  \n",
       "1               8.16  \n",
       "2               6.70  \n",
       "3               6.20  \n",
       "4               6.00  \n",
       "5               5.89  \n",
       "6               5.30  \n",
       "7               5.24  \n",
       "8               4.92  \n",
       "9               4.89  \n",
       "10              4.80  \n",
       "11              4.55  \n",
       "12              4.35  \n",
       "13              3.91  \n",
       "14              3.87  \n",
       "15              3.80  \n",
       "16              3.79  \n",
       "17              3.66  \n",
       "18              3.64  \n",
       "19              3.60  \n",
       "20              3.59  \n",
       "21              3.52  \n",
       "22              3.48  \n",
       "23              3.45  \n",
       "24              3.45  \n",
       "25              3.44  \n",
       "26              3.42  \n",
       "27              3.41  \n",
       "28              3.38  \n",
       "29              3.38  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Novels=pd.DataFrame({'Rank':Rank,'Name': Name,'Artist':Artist,'Upload date':Upload_date,'Views in Billions':Views})\n",
    "Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27caf777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd67fff4",
   "metadata": {},
   "source": [
    "<a id=\"BCCI\"> </a>\n",
    "## 2. Scrape the details teamIndia’s international fixtures from bcci.tv. Url=https://www.bcci.tv/.\n",
    "\n",
    "You need to find following details:\n",
    "\n",
    "    A) Match title (I.e. 1stODI)\n",
    "    B) Series\n",
    "    C) Place\n",
    "    D) Date\n",
    "    E) Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ebbb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3a8f4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the BCCI page on automated chrome browser\n",
    "\n",
    "driver.get('https://www.bcci.tv/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "858650a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating page for international fixtures by xpath\n",
    "\n",
    "option = driver.find_element(By.XPATH,'/html/body/nav/div[1]/button')\n",
    "option.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7aff450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on international by xpath\n",
    "\n",
    "international = driver.find_element(By.XPATH,'/html/body/nav/div[1]/div[2]/ul[1]/li[2]/a') \n",
    "international.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e796070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['ICC WORLD TEST CHAMPIONSHIP FINAL 2023']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Team name\n",
    "\n",
    "Match_title=[]                 \n",
    "\n",
    "match_title=driver.find_elements(By.XPATH,'//h5[@class=\"match-tournament-name ng-binding\"]')\n",
    "for i in match_title:\n",
    "    title=i.text\n",
    "    Match_title.append(title)\n",
    "\n",
    "print(len(Match_title),Match_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69f308d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['ALL FORMATS']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Team name\n",
    "\n",
    "Series=[]                 \n",
    "\n",
    "series=driver.find_elements(By.XPATH,'/html/body/div[2]/div[2]/div/div/div/div[2]/div[2]/div/div[3]/div/div[1]')\n",
    "for i in series:\n",
    "    title=i.text\n",
    "    Series.append(title)\n",
    "\n",
    "print(len(Series),Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a22d0e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['Final - Kennington Oval, London']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Place\n",
    "\n",
    "Place=[]                 \n",
    "\n",
    "place=driver.find_elements(By.XPATH,'//div[@class=\"match-place ng-scope\"]')\n",
    "for i in place:\n",
    "    title=i.text\n",
    "    Place.append(title)\n",
    "\n",
    "print(len(Place),Place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c4b2f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['7 JUN 2023']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Date\n",
    "\n",
    "Date=[]                 \n",
    "\n",
    "date=driver.find_elements(By.XPATH,'//div[@class=\"match-dates ng-binding\"]')\n",
    "for i in date:\n",
    "    title=i.text\n",
    "    Date.append(title)\n",
    "\n",
    "print(len(Date),Date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53ee83d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ['3:30 PM IST']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Time\n",
    "\n",
    "Time=[]                 \n",
    "\n",
    "time=driver.find_elements(By.XPATH,'//div[@class=\"match-time no-margin ng-binding\"]')\n",
    "for i in time:\n",
    "    title=i.text\n",
    "    Time.append(title)\n",
    "\n",
    "print(len(Time),Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc63a70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match Title</th>\n",
       "      <th>Series</th>\n",
       "      <th>Place</th>\n",
       "      <th>Date</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ICC WORLD TEST CHAMPIONSHIP FINAL 2023</td>\n",
       "      <td>ALL FORMATS</td>\n",
       "      <td>Final - Kennington Oval, London</td>\n",
       "      <td>7 JUN 2023</td>\n",
       "      <td>3:30 PM IST</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Match Title       Series  \\\n",
       "0  ICC WORLD TEST CHAMPIONSHIP FINAL 2023  ALL FORMATS   \n",
       "\n",
       "                             Place        Date         Time  \n",
       "0  Final - Kennington Oval, London  7 JUN 2023  3:30 PM IST  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Novels=pd.DataFrame({'Match Title':Match_title,'Series': Series,'Place':Place,'Date':Date,'Time':Time})\n",
    "Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1bfa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7954cb3",
   "metadata": {},
   "source": [
    "<a id=\"statistics\"> </a>\n",
    "## 3. Scrape the details of State-wise GDP ofIndia fromstatisticstime.com.  Url = http://statisticstimes.com/\n",
    "\n",
    "You have to find following details:\n",
    "\n",
    "    A) Rank\n",
    "    B) State\n",
    "    C) GSDP(18-19)- at current prices\n",
    "    D) GSDP(19-20)- at current prices\n",
    "    E) Share(18-19)\n",
    "    F) GDP($ billion )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "272e0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9d03975f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the statistics page on automated chrome browser\n",
    "\n",
    "driver.get('http://statisticstimes.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf016c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on economy\n",
    "\n",
    "economy = driver.find_element(By.XPATH,'/html/body/div[2]/div[1]/div[2]/div[2]') \n",
    "economy.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8041f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on india\n",
    "\n",
    "india = driver.find_element(By.XPATH,'//*[@id=\"top\"]/div[2]/div[2]/div/a[3]')\n",
    "india.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76c27502",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on gtp of indian states\n",
    "\n",
    "gdp = driver.find_element(By.XPATH,'/html/body/div[2]/div[2]/div[2]/ul/li[1]/a')\n",
    "gdp.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d65a65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Rank \n",
    "\n",
    "Rank=[]\n",
    "\n",
    "rank=driver.find_elements(By.XPATH,'//td[@class=\"data1\"]')\n",
    "for i in rank[0:33]:\n",
    "    title=i.text\n",
    "    Rank.append(title)\n",
    "\n",
    "print(len(Rank),Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47fc81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['Maharashtra', 'Tamil Nadu', 'Uttar Pradesh', 'Gujarat', 'Karnataka', 'West Bengal', 'Rajasthan', 'Andhra Pradesh', 'Telangana', 'Madhya Pradesh', 'Kerala', 'Delhi', 'Haryana', 'Bihar', 'Punjab', 'Odisha', 'Assam', 'Chhattisgarh', 'Jharkhand', 'Uttarakhand', 'Jammu & Kashmir', 'Himachal Pradesh', 'Goa', 'Tripura', 'Chandigarh', 'Puducherry', 'Meghalaya', 'Sikkim', 'Manipur', 'Nagaland', 'Arunachal Pradesh', 'Mizoram', 'Andaman & Nicobar Islands']\n"
     ]
    }
   ],
   "source": [
    "#scraping the State \n",
    "\n",
    "State=[]\n",
    "\n",
    "state=driver.find_elements(By.XPATH,'//td[@class=\"name\"]')\n",
    "for i in state[0:33]:\n",
    "    title=i.text\n",
    "    State.append(title)\n",
    "\n",
    "print(len(State),State)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0faeff2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['2,632,792', '1,630,208', '1,584,764', '1,502,899', '1,493,127', '1,089,898', '942,586', '862,957', '861,031', '809,592', '781,653', '774,870', '734,163', '530,363', '526,376', '487,805', '315,881', '304,063', '297,204', '245,895', '155,956', '153,845', '73,170', '49,845', '42,114', '34,433', '33,481', '28,723', '27,870', '27,283', '24,603', '22,287', '-']\n"
     ]
    }
   ],
   "source": [
    "#GSDP(18-19)- at current prices\n",
    "\n",
    "GSDP_Current1819=[]\n",
    "\n",
    "gsdp_current=driver.find_elements(By.XPATH,'//td[@class=\"data sorting_1\"]')\n",
    "for i in gsdp_current[0:33]:\n",
    "    title=i.text\n",
    "    GSDP_Current1819.append(title)\n",
    "\n",
    "print(len(GSDP_Current1819),GSDP_Current1819)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1201bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['-', '1,845,853', '1,687,818', '-', '1,631,977', '1,253,832', '1,020,989', '972,782', '969,604', '906,672', '-', '856,112', '831,610', '611,804', '574,760', '521,275', '-', '329,180', '328,598', '-', '-', '165,472', '80,449', '55,984', '-', '38,253', '36,572', '32,496', '31,790', '-', '-', '26,503', '-']\n"
     ]
    }
   ],
   "source": [
    "#GSDP(19-20)- at current prices\n",
    "\n",
    "GSDP_Current1920=[]\n",
    "\n",
    "gsdp_current=driver.find_elements(By.XPATH,'//*[@id=\"table_id\"]/tbody/tr/td[3]')\n",
    "for i in gsdp_current[0:33]:\n",
    "    title=i.text\n",
    "    GSDP_Current1920.append(title)\n",
    "\n",
    "print(len(GSDP_Current1920),GSDP_Current1920)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0215b000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['13.94%', '8.63%', '8.39%', '7.96%', '7.91%', '5.77%', '4.99%', '4.57%', '4.56%', '4.29%', '4.14%', '4.10%', '3.89%', '2.81%', '2.79%', '2.58%', '1.67%', '1.61%', '1.57%', '1.30%', '0.83%', '0.81%', '0.39%', '0.26%', '0.22%', '0.18%', '0.18%', '0.15%', '0.15%', '0.14%', '0.13%', '0.12%', '-']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Share (18-19)\n",
    "\n",
    "Share=[]\n",
    "\n",
    "share=driver.find_elements(By.XPATH,'//div[@class=\"dataTables_wrapper\"]/table/tbody/tr/td[5]')\n",
    "for i in share[0:33]:\n",
    "    title=i.text\n",
    "    Share.append(title)\n",
    "\n",
    "print(len(Share),Share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07aa642c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 ['399.921', '247.629', '240.726', '228.290', '226.806', '165.556', '143.179', '131.083', '130.791', '122.977', '118.733', '117.703', '111.519', '80.562', '79.957', '74.098', '47.982', '46.187', '45.145', '37.351', '23.690', '23.369', '11.115', '7.571', '6.397', '5.230', '5.086', '4.363', '4.233', '4.144', '3.737', '3.385', '-']\n"
     ]
    }
   ],
   "source": [
    "#scraping the GDP($ billion )\n",
    "\n",
    "GDP=[]\n",
    "\n",
    "gdp=driver.find_elements(By.XPATH,'//div[@class=\"dataTables_wrapper\"]/table/tbody/tr/td[6]')\n",
    "for i in gdp[0:33]:\n",
    "    title=i.text\n",
    "    GDP.append(title)\n",
    "\n",
    "print(len(GDP),GDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8fa2cb79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>State</th>\n",
       "      <th>GSDP Current (18-19)</th>\n",
       "      <th>GSDP Current (19-20)</th>\n",
       "      <th>Share (18-19)</th>\n",
       "      <th>GDP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Maharashtra</td>\n",
       "      <td>2,632,792</td>\n",
       "      <td>-</td>\n",
       "      <td>13.94%</td>\n",
       "      <td>399.921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Tamil Nadu</td>\n",
       "      <td>1,630,208</td>\n",
       "      <td>1,845,853</td>\n",
       "      <td>8.63%</td>\n",
       "      <td>247.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Uttar Pradesh</td>\n",
       "      <td>1,584,764</td>\n",
       "      <td>1,687,818</td>\n",
       "      <td>8.39%</td>\n",
       "      <td>240.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Gujarat</td>\n",
       "      <td>1,502,899</td>\n",
       "      <td>-</td>\n",
       "      <td>7.96%</td>\n",
       "      <td>228.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Karnataka</td>\n",
       "      <td>1,493,127</td>\n",
       "      <td>1,631,977</td>\n",
       "      <td>7.91%</td>\n",
       "      <td>226.806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>West Bengal</td>\n",
       "      <td>1,089,898</td>\n",
       "      <td>1,253,832</td>\n",
       "      <td>5.77%</td>\n",
       "      <td>165.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Rajasthan</td>\n",
       "      <td>942,586</td>\n",
       "      <td>1,020,989</td>\n",
       "      <td>4.99%</td>\n",
       "      <td>143.179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Andhra Pradesh</td>\n",
       "      <td>862,957</td>\n",
       "      <td>972,782</td>\n",
       "      <td>4.57%</td>\n",
       "      <td>131.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Telangana</td>\n",
       "      <td>861,031</td>\n",
       "      <td>969,604</td>\n",
       "      <td>4.56%</td>\n",
       "      <td>130.791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Madhya Pradesh</td>\n",
       "      <td>809,592</td>\n",
       "      <td>906,672</td>\n",
       "      <td>4.29%</td>\n",
       "      <td>122.977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Kerala</td>\n",
       "      <td>781,653</td>\n",
       "      <td>-</td>\n",
       "      <td>4.14%</td>\n",
       "      <td>118.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>774,870</td>\n",
       "      <td>856,112</td>\n",
       "      <td>4.10%</td>\n",
       "      <td>117.703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>Haryana</td>\n",
       "      <td>734,163</td>\n",
       "      <td>831,610</td>\n",
       "      <td>3.89%</td>\n",
       "      <td>111.519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Bihar</td>\n",
       "      <td>530,363</td>\n",
       "      <td>611,804</td>\n",
       "      <td>2.81%</td>\n",
       "      <td>80.562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Punjab</td>\n",
       "      <td>526,376</td>\n",
       "      <td>574,760</td>\n",
       "      <td>2.79%</td>\n",
       "      <td>79.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Odisha</td>\n",
       "      <td>487,805</td>\n",
       "      <td>521,275</td>\n",
       "      <td>2.58%</td>\n",
       "      <td>74.098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Assam</td>\n",
       "      <td>315,881</td>\n",
       "      <td>-</td>\n",
       "      <td>1.67%</td>\n",
       "      <td>47.982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Chhattisgarh</td>\n",
       "      <td>304,063</td>\n",
       "      <td>329,180</td>\n",
       "      <td>1.61%</td>\n",
       "      <td>46.187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Jharkhand</td>\n",
       "      <td>297,204</td>\n",
       "      <td>328,598</td>\n",
       "      <td>1.57%</td>\n",
       "      <td>45.145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>Uttarakhand</td>\n",
       "      <td>245,895</td>\n",
       "      <td>-</td>\n",
       "      <td>1.30%</td>\n",
       "      <td>37.351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Jammu &amp; Kashmir</td>\n",
       "      <td>155,956</td>\n",
       "      <td>-</td>\n",
       "      <td>0.83%</td>\n",
       "      <td>23.690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Himachal Pradesh</td>\n",
       "      <td>153,845</td>\n",
       "      <td>165,472</td>\n",
       "      <td>0.81%</td>\n",
       "      <td>23.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Goa</td>\n",
       "      <td>73,170</td>\n",
       "      <td>80,449</td>\n",
       "      <td>0.39%</td>\n",
       "      <td>11.115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>Tripura</td>\n",
       "      <td>49,845</td>\n",
       "      <td>55,984</td>\n",
       "      <td>0.26%</td>\n",
       "      <td>7.571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Chandigarh</td>\n",
       "      <td>42,114</td>\n",
       "      <td>-</td>\n",
       "      <td>0.22%</td>\n",
       "      <td>6.397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Puducherry</td>\n",
       "      <td>34,433</td>\n",
       "      <td>38,253</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Meghalaya</td>\n",
       "      <td>33,481</td>\n",
       "      <td>36,572</td>\n",
       "      <td>0.18%</td>\n",
       "      <td>5.086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Sikkim</td>\n",
       "      <td>28,723</td>\n",
       "      <td>32,496</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>Manipur</td>\n",
       "      <td>27,870</td>\n",
       "      <td>31,790</td>\n",
       "      <td>0.15%</td>\n",
       "      <td>4.233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>Nagaland</td>\n",
       "      <td>27,283</td>\n",
       "      <td>-</td>\n",
       "      <td>0.14%</td>\n",
       "      <td>4.144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>Arunachal Pradesh</td>\n",
       "      <td>24,603</td>\n",
       "      <td>-</td>\n",
       "      <td>0.13%</td>\n",
       "      <td>3.737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>Mizoram</td>\n",
       "      <td>22,287</td>\n",
       "      <td>26,503</td>\n",
       "      <td>0.12%</td>\n",
       "      <td>3.385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>Andaman &amp; Nicobar Islands</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      State GSDP Current (18-19) GSDP Current (19-20)  \\\n",
       "0     1                Maharashtra            2,632,792                    -   \n",
       "1     2                 Tamil Nadu            1,630,208            1,845,853   \n",
       "2     3              Uttar Pradesh            1,584,764            1,687,818   \n",
       "3     4                    Gujarat            1,502,899                    -   \n",
       "4     5                  Karnataka            1,493,127            1,631,977   \n",
       "5     6                West Bengal            1,089,898            1,253,832   \n",
       "6     7                  Rajasthan              942,586            1,020,989   \n",
       "7     8             Andhra Pradesh              862,957              972,782   \n",
       "8     9                  Telangana              861,031              969,604   \n",
       "9    10             Madhya Pradesh              809,592              906,672   \n",
       "10   11                     Kerala              781,653                    -   \n",
       "11   12                      Delhi              774,870              856,112   \n",
       "12   13                    Haryana              734,163              831,610   \n",
       "13   14                      Bihar              530,363              611,804   \n",
       "14   15                     Punjab              526,376              574,760   \n",
       "15   16                     Odisha              487,805              521,275   \n",
       "16   17                      Assam              315,881                    -   \n",
       "17   18               Chhattisgarh              304,063              329,180   \n",
       "18   19                  Jharkhand              297,204              328,598   \n",
       "19   20                Uttarakhand              245,895                    -   \n",
       "20   21            Jammu & Kashmir              155,956                    -   \n",
       "21   22           Himachal Pradesh              153,845              165,472   \n",
       "22   23                        Goa               73,170               80,449   \n",
       "23   24                    Tripura               49,845               55,984   \n",
       "24   25                 Chandigarh               42,114                    -   \n",
       "25   26                 Puducherry               34,433               38,253   \n",
       "26   27                  Meghalaya               33,481               36,572   \n",
       "27   28                     Sikkim               28,723               32,496   \n",
       "28   29                    Manipur               27,870               31,790   \n",
       "29   30                   Nagaland               27,283                    -   \n",
       "30   31          Arunachal Pradesh               24,603                    -   \n",
       "31   32                    Mizoram               22,287               26,503   \n",
       "32   33  Andaman & Nicobar Islands                    -                    -   \n",
       "\n",
       "   Share (18-19)      GDP  \n",
       "0         13.94%  399.921  \n",
       "1          8.63%  247.629  \n",
       "2          8.39%  240.726  \n",
       "3          7.96%  228.290  \n",
       "4          7.91%  226.806  \n",
       "5          5.77%  165.556  \n",
       "6          4.99%  143.179  \n",
       "7          4.57%  131.083  \n",
       "8          4.56%  130.791  \n",
       "9          4.29%  122.977  \n",
       "10         4.14%  118.733  \n",
       "11         4.10%  117.703  \n",
       "12         3.89%  111.519  \n",
       "13         2.81%   80.562  \n",
       "14         2.79%   79.957  \n",
       "15         2.58%   74.098  \n",
       "16         1.67%   47.982  \n",
       "17         1.61%   46.187  \n",
       "18         1.57%   45.145  \n",
       "19         1.30%   37.351  \n",
       "20         0.83%   23.690  \n",
       "21         0.81%   23.369  \n",
       "22         0.39%   11.115  \n",
       "23         0.26%    7.571  \n",
       "24         0.22%    6.397  \n",
       "25         0.18%    5.230  \n",
       "26         0.18%    5.086  \n",
       "27         0.15%    4.363  \n",
       "28         0.15%    4.233  \n",
       "29         0.14%    4.144  \n",
       "30         0.13%    3.737  \n",
       "31         0.12%    3.385  \n",
       "32             -        -  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Novels=pd.DataFrame({'Rank':Rank,'State': State,'GSDP Current (18-19)':GSDP_Current1819,'GSDP Current (19-20)':GSDP_Current1920,'Share (18-19)':Share,'GDP':GDP})\n",
    "Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f20b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a8fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d57e53f",
   "metadata": {},
   "source": [
    "<a id=\"Github\"> </a>\n",
    "## 4. Scrape the details of trending repositories on Github.com. Url = https://github.com/.\n",
    "\n",
    "You have to find the following details:\n",
    "    \n",
    "    A) Repository title\n",
    "    B) Repository description\n",
    "    C) Contributors count\n",
    "    D) Language used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ea61250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b210bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the github page on automated chrome browser\n",
    "\n",
    "driver.get('http://github.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "71c59c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click\n",
    "\n",
    "option = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[1]/div[2]/button/span') \n",
    "option.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d5b58cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on open source\n",
    "\n",
    "open_source = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]') \n",
    "open_source.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1485a40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on trending\n",
    "\n",
    "trending = driver.find_element(By.XPATH,'/html/body/div[1]/div[1]/header/div/div[2]/div/nav/ul/li[3]/div/div[3]/ul/li[2]/a')\n",
    "trending.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44612e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['artidoro /', 'geohot /', 'JiauZhang /', 'libarchive /', 'btw-so /', 'microsoft /', 'JushBJJ /', 'microsoft /', 'kamiyaa /', 'strapi /', 'Zeqiang-Lai /', 'kyegomez /', 'dalathegreat /', 'salesforce /', 'drboog /', 'pengzhile /', 'Cyber-Security-Hub /', 'chinese-poetry /', 'immersive-translate /', 'homanp /']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Repository Title\n",
    "\n",
    "Repository_Title=[]\n",
    "\n",
    "rep_title=driver.find_elements(By.XPATH,'//span[@class=\"text-normal\"]')\n",
    "for i in rep_title[0:20]:\n",
    "    title=i.text\n",
    "    Repository_Title.append(title)\n",
    "\n",
    "print(len(Repository_Title),Repository_Title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2f0e7f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['QLoRA: Efficient Finetuning of Quantized LLMs', 'You like pytorch? You like micrograd? You love tinygrad! ❤️', 'Implementation of DragGAN: Interactive Point-based Manipulation on the Generative Image Manifold', 'Multi-format archive and compression library', 'List of open-source alternatives to everyday SaaS products.', 'A GPT-4 AI Tutor Prompt for customizable personalized learning experiences.', 'Windows Dev Home Application', 'ranger-like terminal file manager written in Rust', 'Strapi Demo application for Corporate Websites using Next.js', 'Online Demo and Implementation of DragGAN - \"Drag Your GAN: Interactive Point-based Manipulation on the Generative Image Manifold\" （DragGAN 全功能实现，在线Demo，本地部署试用，代码、模型已全部开源）', 'Plug in and Play Implementation of Tree of Thoughts: Deliberate Problem Solving with Large Language Models that Elevates Model Reasoning by atleast 70%', 'This software converts the LEAF CAN into Modbus RTU registers understood by solar inverters that take the BYD 11kWh HVM battery', 'LAVIS - A One-stop Library for Language-Vision Intelligence', 'Code for Enhancing Detail Preservation for Customized Text-to-Image Generation: A Regularization-Free Approach', '潘多拉，一个让你呼吸顺畅的ChatGPT。Pandora, a ChatGPT that helps you breathe smoothly.', 'Cyber Security Trainings', 'The most comprehensive database of Chinese poetry 🧶最全中华古诗词数据库, 唐宋两朝近一万四千古诗人, 接近5.5万首唐诗加26万宋诗. 两宋时期1564位词人，21050首词。', 'Immersive Dual Web Page Translation Extension - 沉浸式双语网页翻译扩展', '🥷 SuperAgent - Deploy LLM Agents to production', 'A visual no-code/code-free web crawler/spider一个可视化爬虫软件，可以无代码图形化设计和执行的爬虫任务']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Repository description\n",
    "\n",
    "Repository_Description=[]\n",
    "\n",
    "rep_description=driver.find_elements(By.XPATH,'//p[@class=\"col-9 color-fg-muted my-1 pr-4\"]')\n",
    "for i in rep_description[0:20]:\n",
    "    title=i.text\n",
    "    Repository_Description.append(title)\n",
    "\n",
    "print(len(Repository_Description),Repository_Description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "006e7635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['162', '1,191', '237', '689', '56', '19', '749', '93', '114', '20', '224', '160', '38', '454', '15', '1,653', '11', '8,348', '381', '55']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Contributors count\n",
    "\n",
    "Contributors_count=[]\n",
    "\n",
    "Contr_count=driver.find_elements(By.XPATH,'//div[@class=\"f6 color-fg-muted mt-2\"]/a[2]')\n",
    "for i in Contr_count[0:20]:\n",
    "    title=i.text\n",
    "    Contributors_count.append(title)\n",
    "\n",
    "print(len(Contributors_count),Contributors_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "716a067a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 ['Jupyter Notebook', 'Python', 'Python', 'C', 'C#', 'Rust', 'TypeScript', 'Python', 'Python', 'C++', 'Python', 'Jupyter Notebook', 'Python', 'SCSS', 'JavaScript', 'TypeScript', 'Python', 'JavaScript', 'Python', 'JavaScript']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Language used\n",
    "\n",
    "Language_used=[]\n",
    "\n",
    "language_used=driver.find_elements(By.XPATH,'//div[@class=\"f6 color-fg-muted mt-2\"]/span/span[2]')\n",
    "for i in language_used[0:20]:\n",
    "    title=i.text\n",
    "    Language_used.append(title)\n",
    "\n",
    "print(len(Language_used),Language_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "090dce00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Repository Title</th>\n",
       "      <th>Repository Description</th>\n",
       "      <th>Contributors Count</th>\n",
       "      <th>Language Used</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>artidoro /</td>\n",
       "      <td>QLoRA: Efficient Finetuning of Quantized LLMs</td>\n",
       "      <td>162</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>geohot /</td>\n",
       "      <td>You like pytorch? You like micrograd? You love...</td>\n",
       "      <td>1,191</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JiauZhang /</td>\n",
       "      <td>Implementation of DragGAN: Interactive Point-b...</td>\n",
       "      <td>237</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>libarchive /</td>\n",
       "      <td>Multi-format archive and compression library</td>\n",
       "      <td>689</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>btw-so /</td>\n",
       "      <td>List of open-source alternatives to everyday S...</td>\n",
       "      <td>56</td>\n",
       "      <td>C#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>microsoft /</td>\n",
       "      <td>A GPT-4 AI Tutor Prompt for customizable perso...</td>\n",
       "      <td>19</td>\n",
       "      <td>Rust</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>JushBJJ /</td>\n",
       "      <td>Windows Dev Home Application</td>\n",
       "      <td>749</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>microsoft /</td>\n",
       "      <td>ranger-like terminal file manager written in Rust</td>\n",
       "      <td>93</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>kamiyaa /</td>\n",
       "      <td>Strapi Demo application for Corporate Websites...</td>\n",
       "      <td>114</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>strapi /</td>\n",
       "      <td>Online Demo and Implementation of DragGAN - \"D...</td>\n",
       "      <td>20</td>\n",
       "      <td>C++</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Zeqiang-Lai /</td>\n",
       "      <td>Plug in and Play Implementation of Tree of Tho...</td>\n",
       "      <td>224</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>kyegomez /</td>\n",
       "      <td>This software converts the LEAF CAN into Modbu...</td>\n",
       "      <td>160</td>\n",
       "      <td>Jupyter Notebook</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>dalathegreat /</td>\n",
       "      <td>LAVIS - A One-stop Library for Language-Vision...</td>\n",
       "      <td>38</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>salesforce /</td>\n",
       "      <td>Code for Enhancing Detail Preservation for Cus...</td>\n",
       "      <td>454</td>\n",
       "      <td>SCSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>drboog /</td>\n",
       "      <td>潘多拉，一个让你呼吸顺畅的ChatGPT。Pandora, a ChatGPT that h...</td>\n",
       "      <td>15</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>pengzhile /</td>\n",
       "      <td>Cyber Security Trainings</td>\n",
       "      <td>1,653</td>\n",
       "      <td>TypeScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Cyber-Security-Hub /</td>\n",
       "      <td>The most comprehensive database of Chinese poe...</td>\n",
       "      <td>11</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>chinese-poetry /</td>\n",
       "      <td>Immersive Dual Web Page Translation Extension ...</td>\n",
       "      <td>8,348</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>immersive-translate /</td>\n",
       "      <td>🥷 SuperAgent - Deploy LLM Agents to production</td>\n",
       "      <td>381</td>\n",
       "      <td>Python</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>homanp /</td>\n",
       "      <td>A visual no-code/code-free web crawler/spider一...</td>\n",
       "      <td>55</td>\n",
       "      <td>JavaScript</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Repository Title                             Repository Description  \\\n",
       "0              artidoro /      QLoRA: Efficient Finetuning of Quantized LLMs   \n",
       "1                geohot /  You like pytorch? You like micrograd? You love...   \n",
       "2             JiauZhang /  Implementation of DragGAN: Interactive Point-b...   \n",
       "3            libarchive /       Multi-format archive and compression library   \n",
       "4                btw-so /  List of open-source alternatives to everyday S...   \n",
       "5             microsoft /  A GPT-4 AI Tutor Prompt for customizable perso...   \n",
       "6               JushBJJ /                       Windows Dev Home Application   \n",
       "7             microsoft /  ranger-like terminal file manager written in Rust   \n",
       "8               kamiyaa /  Strapi Demo application for Corporate Websites...   \n",
       "9                strapi /  Online Demo and Implementation of DragGAN - \"D...   \n",
       "10          Zeqiang-Lai /  Plug in and Play Implementation of Tree of Tho...   \n",
       "11             kyegomez /  This software converts the LEAF CAN into Modbu...   \n",
       "12         dalathegreat /  LAVIS - A One-stop Library for Language-Vision...   \n",
       "13           salesforce /  Code for Enhancing Detail Preservation for Cus...   \n",
       "14               drboog /  潘多拉，一个让你呼吸顺畅的ChatGPT。Pandora, a ChatGPT that h...   \n",
       "15            pengzhile /                           Cyber Security Trainings   \n",
       "16   Cyber-Security-Hub /  The most comprehensive database of Chinese poe...   \n",
       "17       chinese-poetry /  Immersive Dual Web Page Translation Extension ...   \n",
       "18  immersive-translate /     🥷 SuperAgent - Deploy LLM Agents to production   \n",
       "19               homanp /  A visual no-code/code-free web crawler/spider一...   \n",
       "\n",
       "   Contributors Count     Language Used  \n",
       "0                 162  Jupyter Notebook  \n",
       "1               1,191            Python  \n",
       "2                 237            Python  \n",
       "3                 689                 C  \n",
       "4                  56                C#  \n",
       "5                  19              Rust  \n",
       "6                 749        TypeScript  \n",
       "7                  93            Python  \n",
       "8                 114            Python  \n",
       "9                  20               C++  \n",
       "10                224            Python  \n",
       "11                160  Jupyter Notebook  \n",
       "12                 38            Python  \n",
       "13                454              SCSS  \n",
       "14                 15        JavaScript  \n",
       "15              1,653        TypeScript  \n",
       "16                 11            Python  \n",
       "17              8,348        JavaScript  \n",
       "18                381            Python  \n",
       "19                 55        JavaScript  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Repository=pd.DataFrame({'Repository Title':Repository_Title,'Repository Description':Repository_Description,'Contributors Count':Contributors_count,'Language Used':Language_used})\n",
    "Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71064a4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb63b143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26ca87d5",
   "metadata": {},
   "source": [
    "<a id=\"billiboard\"> </a>\n",
    "## 5. Scrape the details of top 100 songs on billiboard.com.  Url = https:/www.billboard.com/.\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "    A) Song name\n",
    "    B) Artist name\n",
    "    C) Last week rank\n",
    "    D) Peak rank\n",
    "    E) Weeks on board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d617c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "276d26d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the github page on automated chrome browser\n",
    "\n",
    "driver.get('https://www.billboard.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c5cdf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click\n",
    "\n",
    "option = driver.find_element(By.XPATH,'/html/body/div[3]/header/div/div[4]/div/div[1]/div[1]/button') \n",
    "option.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e5398c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Click on charts + icon\n",
    "\n",
    "charts = driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div/div/div/ul/li[1]/h3/button') \n",
    "charts.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dff69dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 100 songs\n",
    "\n",
    "top100 = driver.find_element(By.XPATH,'/html/body/div[3]/div[9]/div/div/div/ul/li[1]/ul/li[2]')\n",
    "top100.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9cc7b56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Last Night', 'All My Life', 'Flowers', 'Kill Bill', 'Ella Baila Sola', 'Calm Down', \"Creepin'\", 'Favorite Song', 'Fast Car', 'Anti-Hero', 'Die For You', 'Un x100to', \"Boy's A Liar, Pt. 2\", 'Sure Thing', 'Rock And A Hard Place', 'La Bebe', 'Search & Rescue', 'Cupid', 'You Proof', 'Something In The Orange', 'As It Was', 'Chemical', \"Dancin' In The Country\", 'Players', \"Thinkin' Bout Me\", 'Eyes Closed', \"I'm Good (Blue)\", 'Thank God', 'Snooze', 'One Thing At A Time', 'Thought You Should Know', 'Karma', 'Area Codes', 'Under The Influence', 'Tennessee Orange', 'Por Las Noches', 'Next Thing You Know', 'Princess Diana', 'PRC', 'Religiously', 'Spin Bout U', 'Unholy', 'Just Wanna Rock', 'Love You Anyway', 'Rich Flex', 'See You Again', 'Mathematical Disrespect', 'Wild As Her', 'TQG', 'Superhero (Heroes & Villains)', 'Slut Me Out', 'Daylight', 'Handle On You', 'AMG', 'El Azul', 'Need A Favor', 'Waffle House', 'Double Fantasy', 'Nonsense', 'Chanel', 'Human', \"Bitch Let's Do It\", 'Memory Lane', 'ICU', 'It Matters To Her', 'Life Goes On', 'Cowgirls', \"Ain't That Some\", 'Fragil', 'Fight The Feeling', 'Low Down', 'El Gordo Trae El Mando', 'Red Ruby Da Sleeze', 'Shake Sumn', 'Yandel 150', 'Trance', 'Beso', 'Painting Pictures', 'I Wrote The Book', 'What It Is (Block Boy)', 'Go Hard', 'Everything I Love', 'Moonlight', 'Acrostico', 'Bury Me In Georgia', 'Man Made A Bar', 'F**k The Industry Pt. 2', 'Ch y La Pizza', 'People', 'Igualito A Mi Apa', 'You, Me, & Whiskey', 'Ceilings', 'In Ha Mood', \"Everything She Ain't\", 'Your Heart Or Mine', 'Forever', 'Private Landing', 'I Heard', 'Sunrise', 'Happy']\n"
     ]
    }
   ],
   "source": [
    "#scraping the song name\n",
    "\n",
    "Song_name=[]\n",
    "\n",
    "song_name=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li/h3')\n",
    "for i in song_name:\n",
    "    title=i.text\n",
    "    Song_name.append(title)\n",
    "\n",
    "print(len(Song_name),Song_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "08914807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Morgan Wallen', 'Lil Durk Featuring J. Cole', 'Miley Cyrus', 'SZA', 'Eslabon Armado X Peso Pluma', 'Rema & Selena Gomez', 'Metro Boomin, The Weeknd & 21 Savage', 'Toosii', 'Luke Combs', 'Taylor Swift', 'The Weeknd & Ariana Grande', 'Grupo Frontera X Bad Bunny', 'PinkPantheress & Ice Spice', 'Miguel', 'Bailey Zimmerman', 'Yng Lvcas x Peso Pluma', 'Drake', 'Fifty Fifty', 'Morgan Wallen', 'Zach Bryan', 'Harry Styles', 'Post Malone', 'Tyler Hubbard', 'Coi Leray', 'Morgan Wallen', 'Ed Sheeran', 'David Guetta & Bebe Rexha', 'Kane Brown With Katelyn Brown', 'SZA', 'Morgan Wallen', 'Morgan Wallen', 'Taylor Swift', 'Kali', 'Chris Brown', 'Megan Moroney', 'Peso Pluma', 'Jordan Davis', 'Ice Spice & Nicki Minaj', 'Peso Pluma X Natanael Cano', 'Bailey Zimmerman', 'Drake & 21 Savage', 'Sam Smith & Kim Petras', 'Lil Uzi Vert', 'Luke Combs', 'Drake & 21 Savage', 'Tyler, The Creator Featuring Kali Uchis', 'Lil Mabu', 'Corey Kent', 'Karol G x Shakira', 'Metro Boomin, Future & Chris Brown', 'NLE Choppa', 'David Kushner', 'Parker McCollum', 'Gabito Ballesteros, Peso Pluma & Natanael Cano', 'Junior H x Peso Pluma', 'Jelly Roll', 'Jonas Brothers', 'The Weeknd Featuring Future', 'Sabrina Carpenter', 'Becky G & Peso Pluma', 'Cody Johnson', 'YoungBoy Never Broke Again', 'Old Dominion', 'Coco Jones', 'Scotty McCreery', 'Ed Sheeran Featuring Luke Combs', 'Morgan Wallen Featuring ERNEST', 'Morgan Wallen', 'Yahritza y Su Esencia x Grupo Frontera', 'Rod Wave', 'Lil Baby', 'Chino Pacas', 'Nicki Minaj', 'DaBaby', 'Yandel & Feid', 'Metro Boomin, Travis Scott & Young Thug', 'Rosalia & Rauw Alejandro', 'Superstar Pride', 'Morgan Wallen', 'Doechii Featuring Kodak Black', 'Lil Baby', 'Morgan Wallen', 'Kali Uchis', 'Shakira', 'Kane Brown', 'Morgan Wallen Featuring Eric Church', 'YoungBoy Never Broke Again', 'Fuerza Regida X Natanael Cano', 'Libianca', 'Fuerza Regida & Peso Pluma', 'Justin Moore & Priscilla Block', 'Lizzy McAlpine', 'Ice Spice', 'Hailey Whitters', 'Jon Pardi', 'Lil Baby Featuring Fridayy', 'Don Toliver Featuring Justin Bieber & Future', 'YoungBoy Never Broke Again', 'Morgan Wallen', 'NF']\n"
     ]
    }
   ],
   "source": [
    "#scraping the artist name\n",
    "\n",
    "Artist_name=[]\n",
    "\n",
    "artist_name=driver.find_elements(By.XPATH,'//div[@class=\"o-chart-results-list-row-container\"]/ul/li[4]/ul/li/span')\n",
    "for i in artist_name[0:400:4]:\n",
    "    title=i.text\n",
    "    Artist_name.append(title)\n",
    "\n",
    "print(len(Artist_name),Artist_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d33b163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['1', '-', '3', '2', '4', '5', '6', '8', '11', '10', '9', '7', '12', '14', '16', '13', '15', '17', '18', '22', '23', '21', '25', '20', '24', '19', '26', '28', '31', '27', '29', '35', '54', '32', '30', '34', '39', '33', '36', '66', '42', '37', '38', '52', '48', '46', '90', '40', '44', '43', '41', '53', '47', '45', '56', '62', '82', '51', '60', '55', '64', '-', '70', '67', '74', '-', '61', '65', '72', '68', '63', '58', '69', '92', '77', '76', '71', '78', '75', '98', '59', '79', '86', '-', '-', '81', '-', '80', '93', '85', '88', '91', '84', '-', '-', '87', '100', '-', '89', '95']\n"
     ]
    }
   ],
   "source": [
    "#scraping the last weak rank\n",
    "\n",
    "Last_Weak_Rank=[]\n",
    "\n",
    "last_weak_rank=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p u-hidden@tablet\"]//ul/li[3]')\n",
    "for i in last_weak_rank:\n",
    "    title=i.text\n",
    "    Last_Weak_Rank.append(title)\n",
    "\n",
    "print(len(Last_Weak_Rank),Last_Weak_Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "01dff0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['1', '2', '1', '1', '4', '5', '3', '8', '9', '1', '1', '5', '3', '14', '10', '11', '2', '17', '5', '10', '1', '13', '23', '9', '9', '19', '4', '13', '29', '10', '7', '9', '33', '12', '30', '28', '37', '4', '33', '40', '5', '1', '10', '15', '2', '44', '47', '40', '7', '8', '28', '47', '30', '37', '55', '56', '57', '18', '56', '55', '61', '62', '63', '63', '65', '66', '40', '11', '69', '16', '50', '58', '13', '74', '71', '42', '52', '25', '18', '80', '59', '14', '83', '84', '85', '15', '87', '68', '89', '80', '88', '54', '58', '94', '95', '8', '72', '98', '30', '54']\n"
     ]
    }
   ],
   "source": [
    "#scraping the peak rank\n",
    "\n",
    "Peak_Rank=[]\n",
    "\n",
    "peak_rank=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p u-hidden@tablet\"]//ul/li[4]')\n",
    "for i in peak_rank:\n",
    "    title=i.text\n",
    "    Peak_Rank.append(title)\n",
    "\n",
    "print(len(Peak_Rank),Peak_Rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ffd99912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['16', '1', '18', '23', '9', '37', '24', '13', '8', '30', '42', '5', '15', '42', '49', '9', '6', '9', '53', '56', '59', '5', '13', '20', '11', '8', '38', '36', '23', '24', '40', '10', '2', '36', '23', '10', '17', '5', '14', '2', '28', '34', '31', '14', '28', '5', '2', '32', '12', '24', '9', '5', '20', '17', '6', '7', '3', '4', '18', '5', '16', '1', '7', '9', '5', '1', '11', '11', '4', '7', '9', '9', '11', '2', '14', '15', '8', '14', '16', '2', '2', '16', '6', '1', '1', '11', '1', '8', '3', '5', '2', '13', '15', '1', '1', '19', '6', '1', '11', '6']\n"
     ]
    }
   ],
   "source": [
    "#scraping the weeks on board\n",
    "\n",
    "Weeks_on_Board=[]\n",
    "\n",
    "weeks_on_board=driver.find_elements(By.XPATH,'//li[@class=\"lrv-u-width-100p u-hidden@tablet\"]//ul/li[5]')\n",
    "for i in weeks_on_board:\n",
    "    title=i.text\n",
    "    Weeks_on_Board.append(title)\n",
    "\n",
    "print(len(Weeks_on_Board),Weeks_on_Board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46e526c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song name</th>\n",
       "      <th>Artist name</th>\n",
       "      <th>Last Weak Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on Board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last Night</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>All My Life</td>\n",
       "      <td>Lil Durk Featuring J. Cole</td>\n",
       "      <td>-</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Flowers</td>\n",
       "      <td>Miley Cyrus</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kill Bill</td>\n",
       "      <td>SZA</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ella Baila Sola</td>\n",
       "      <td>Eslabon Armado X Peso Pluma</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Forever</td>\n",
       "      <td>Lil Baby Featuring Fridayy</td>\n",
       "      <td>87</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Private Landing</td>\n",
       "      <td>Don Toliver Featuring Justin Bieber &amp; Future</td>\n",
       "      <td>100</td>\n",
       "      <td>72</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>I Heard</td>\n",
       "      <td>YoungBoy Never Broke Again</td>\n",
       "      <td>-</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Sunrise</td>\n",
       "      <td>Morgan Wallen</td>\n",
       "      <td>89</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Happy</td>\n",
       "      <td>NF</td>\n",
       "      <td>95</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Song name                                   Artist name  \\\n",
       "0        Last Night                                 Morgan Wallen   \n",
       "1       All My Life                    Lil Durk Featuring J. Cole   \n",
       "2           Flowers                                   Miley Cyrus   \n",
       "3         Kill Bill                                           SZA   \n",
       "4   Ella Baila Sola                   Eslabon Armado X Peso Pluma   \n",
       "..              ...                                           ...   \n",
       "95          Forever                    Lil Baby Featuring Fridayy   \n",
       "96  Private Landing  Don Toliver Featuring Justin Bieber & Future   \n",
       "97          I Heard                    YoungBoy Never Broke Again   \n",
       "98          Sunrise                                 Morgan Wallen   \n",
       "99            Happy                                            NF   \n",
       "\n",
       "   Last Weak Rank Peak Rank Weeks on Board  \n",
       "0               1         1             16  \n",
       "1               -         2              1  \n",
       "2               3         1             18  \n",
       "3               2         1             23  \n",
       "4               4         4              9  \n",
       "..            ...       ...            ...  \n",
       "95             87         8             19  \n",
       "96            100        72              6  \n",
       "97              -        98              1  \n",
       "98             89        30             11  \n",
       "99             95        54              6  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Billiboard=pd.DataFrame({'Song name':Song_name,'Artist name':Artist_name,'Last Weak Rank':Last_Weak_Rank,'Peak Rank':Peak_Rank,'Weeks on Board':Weeks_on_Board})\n",
    "Billiboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286b518",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55035558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03d9f6b5",
   "metadata": {},
   "source": [
    "<a id=\"novels\"> </a>\n",
    "## 6. Scrape the details of Highest sellingnovels. Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare.\n",
    "\n",
    "You have to find the following details:\n",
    "    \n",
    "    A) Book name\n",
    "    B) Author name\n",
    "    C) Volumes sold\n",
    "    D) Publisher\n",
    "    E) Genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5fc139e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6ddaa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the imdb page on automated chrome browser\n",
    "\n",
    "driver.get('https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "11a81117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Da Vinci Code,The', 'Harry Potter and the Deathly Hallows', \"Harry Potter and the Philosopher's Stone\", 'Harry Potter and the Order of the Phoenix', 'Fifty Shades of Grey', 'Harry Potter and the Goblet of Fire', 'Harry Potter and the Chamber of Secrets', 'Harry Potter and the Prisoner of Azkaban', 'Angels and Demons', \"Harry Potter and the Half-blood Prince:Children's Edition\", 'Fifty Shades Darker', 'Twilight', 'Girl with the Dragon Tattoo,The:Millennium Trilogy', 'Fifty Shades Freed', 'Lost Symbol,The', 'New Moon', 'Deception Point', 'Eclipse', 'Lovely Bones,The', 'Curious Incident of the Dog in the Night-time,The', 'Digital Fortress', 'Short History of Nearly Everything,A', 'Girl Who Played with Fire,The:Millennium Trilogy', 'Breaking Dawn', 'Very Hungry Caterpillar,The:The Very Hungry Caterpillar', 'Gruffalo,The', \"Jamie's 30-Minute Meals\", 'Kite Runner,The', 'One Day', 'Thousand Splendid Suns,A', \"Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\", \"Time Traveler's Wife,The\", 'Atonement', \"Bridget Jones's Diary:A Novel\", 'World According to Clarkson,The', \"Captain Corelli's Mandolin\", 'Sound of Laughter,The', 'Life of Pi', 'Billy Connolly', 'Child Called It,A', \"Gruffalo's Child,The\", \"Angela's Ashes:A Memoir of a Childhood\", 'Birdsong', 'Northern Lights:His Dark Materials S.', 'Labyrinth', 'Harry Potter and the Half-blood Prince', 'Help,The', 'Man and Boy', 'Memoirs of a Geisha', \"No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\", 'Island,The', 'PS, I Love You', 'You are What You Eat:The Plan That Will Change Your Life', 'Shadow of the Wind,The', 'Tales of Beedle the Bard,The', 'Broker,The', \"Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\", 'Subtle Knife,The:His Dark Materials S.', 'Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation', \"Delia's How to Cook:(Bk.1)\", 'Chocolat', 'Boy in the Striped Pyjamas,The', \"My Sister's Keeper\", 'Amber Spyglass,The:His Dark Materials S.', 'To Kill a Mockingbird', 'Men are from Mars, Women are from Venus:A Practical Guide for Improvin', 'Dear Fatty', 'Short History of Tractors in Ukrainian,A', 'Hannibal', 'Lord of the Rings,The', 'Stupid White Men:...and Other Sorry Excuses for the State of the Natio', 'Interpretation of Murder,The', 'Sharon Osbourne Extreme:My Autobiography', 'Alchemist,The:A Fable About Following Your Dream', \"At My Mother's Knee ...:and Other Low Joints\", 'Notes from a Small Island', 'Return of the Naked Chef,The', 'Bridget Jones: The Edge of Reason', \"Jamie's Italy\", 'I Can Make You Thin', 'Down Under', 'Summons,The', 'Small Island', 'Nigella Express', 'Brick Lane', \"Memory Keeper's Daughter,The\", 'Room on the Broom', 'About a Boy', 'My Booky Wook', 'God Delusion,The', '\"Beano\" Annual,The', 'White Teeth', 'House at Riverton,The', 'Book Thief,The', 'Nights of Rain and Stars', 'Ghost,The', 'Happy Days with the Naked Chef', 'Hunger Games,The:Hunger Games Trilogy', \"Lost Boy,The:A Foster Child's Search for the Love of a Family\", \"Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\"]\n"
     ]
    }
   ],
   "source": [
    "#scraping the Books name\n",
    "\n",
    "Books_name=[]\n",
    "\n",
    "books_name=driver.find_elements(By.XPATH,'//tr/td[@class=\"left\"][2]')\n",
    "for i in books_name:\n",
    "    title=i.text\n",
    "    Books_name.append(title)\n",
    "\n",
    "print(len(Books_name),Books_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "65e45044",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Brown, Dan', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'James, E. L.', 'Rowling, J.K.', 'Rowling, J.K.', 'Rowling, J.K.', 'Brown, Dan', 'Rowling, J.K.', 'James, E. L.', 'Meyer, Stephenie', 'Larsson, Stieg', 'James, E. L.', 'Brown, Dan', 'Meyer, Stephenie', 'Brown, Dan', 'Meyer, Stephenie', 'Sebold, Alice', 'Haddon, Mark', 'Brown, Dan', 'Bryson, Bill', 'Larsson, Stieg', 'Meyer, Stephenie', 'Carle, Eric', 'Donaldson, Julia', 'Oliver, Jamie', 'Hosseini, Khaled', 'Nicholls, David', 'Hosseini, Khaled', 'Larsson, Stieg', 'Niffenegger, Audrey', 'McEwan, Ian', 'Fielding, Helen', 'Clarkson, Jeremy', 'Bernieres, Louis de', 'Kay, Peter', 'Martel, Yann', 'Stephenson, Pamela', 'Pelzer, Dave', 'Donaldson, Julia', 'McCourt, Frank', 'Faulks, Sebastian', 'Pullman, Philip', 'Mosse, Kate', 'Rowling, J.K.', 'Stockett, Kathryn', 'Parsons, Tony', 'Golden, Arthur', 'McCall Smith, Alexander', 'Hislop, Victoria', 'Ahern, Cecelia', 'McKeith, Gillian', 'Zafon, Carlos Ruiz', 'Rowling, J.K.', 'Grisham, John', 'Atkins, Robert C.', 'Pullman, Philip', 'Truss, Lynne', 'Smith, Delia', 'Harris, Joanne', 'Boyne, John', 'Picoult, Jodi', 'Pullman, Philip', 'Lee, Harper', 'Gray, John', 'French, Dawn', 'Lewycka, Marina', 'Harris, Thomas', 'Tolkien, J. R. R.', 'Moore, Michael', 'Rubenfeld, Jed', 'Osbourne, Sharon', 'Coelho, Paulo', \"O'Grady, Paul\", 'Bryson, Bill', 'Oliver, Jamie', 'Fielding, Helen', 'Oliver, Jamie', 'McKenna, Paul', 'Bryson, Bill', 'Grisham, John', 'Levy, Andrea', 'Lawson, Nigella', 'Ali, Monica', 'Edwards, Kim', 'Donaldson, Julia', 'Hornby, Nick', 'Brand, Russell', 'Dawkins, Richard', '0', 'Smith, Zadie', 'Morton, Kate', 'Zusak, Markus', 'Binchy, Maeve', 'Harris, Robert', 'Oliver, Jamie', 'Collins, Suzanne', 'Pelzer, Dave', 'Oliver, Jamie']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Author name\n",
    "\n",
    "Author_name=[]\n",
    "\n",
    "author_name=driver.find_elements(By.XPATH,'//tr/td[@class=\"left\"][3]')\n",
    "for i in author_name:\n",
    "    title=i.text\n",
    "    Author_name.append(title)\n",
    "\n",
    "print(len(Author_name),Author_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05cc7387",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['5,094,805', '4,475,152', '4,200,654', '4,179,479', '3,758,936', '3,583,215', '3,484,047', '3,377,906', '3,193,946', '2,950,264', '2,479,784', '2,315,405', '2,233,570', '2,193,928', '2,183,031', '2,152,737', '2,062,145', '2,052,876', '2,005,598', '1,979,552', '1,928,900', '1,852,919', '1,814,784', '1,787,118', '1,783,535', '1,781,269', '1,743,266', '1,629,119', '1,616,068', '1,583,992', '1,555,135', '1,546,886', '1,539,428', '1,508,205', '1,489,403', '1,352,318', '1,310,207', '1,310,176', '1,231,957', '1,217,712', '1,208,711', '1,204,058', '1,184,967', '1,181,503', '1,181,093', '1,153,181', '1,132,336', '1,130,802', '1,126,337', '1,115,549', '1,108,328', '1,107,379', '1,104,403', '1,092,349', '1,090,847', '1,087,262', '1,054,196', '1,037,160', '1,023,688', '1,015,956', '1,009,873', '1,004,414', '1,003,780', '1,002,314', '998,213', '992,846', '986,753', '986,115', '970,509', '967,466', '963,353', '962,515', '959,496', '956,114', '945,640', '931,312', '925,425', '924,695', '906,968', '905,086', '890,847', '869,671', '869,659', '862,602', '856,540', '845,858', '842,535', '828,215', '820,563', '816,907', '816,585', '815,586', '814,370', '809,641', '808,900', '807,311', '794,201', '792,187', '791,507', '791,095']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Volume sold\n",
    "\n",
    "Volume_Sold=[]\n",
    "\n",
    "volume_sold=driver.find_elements(By.XPATH,'//tr/td[@class=\"left\"][4]')\n",
    "for i in volume_sold:\n",
    "    title=i.text\n",
    "    Volume_Sold.append(title)\n",
    "\n",
    "print(len(Volume_Sold),Volume_Sold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df2ee161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Transworld', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Random House', 'Bloomsbury', 'Bloomsbury', 'Bloomsbury', 'Transworld', 'Bloomsbury', 'Random House', 'Little, Brown Book', 'Quercus', 'Random House', 'Transworld', 'Little, Brown Book', 'Transworld', 'Little, Brown Book', 'Pan Macmillan', 'Random House', 'Transworld', 'Transworld', 'Quercus', 'Little, Brown Book', 'Penguin', 'Pan Macmillan', 'Penguin', 'Bloomsbury', 'Hodder & Stoughton', 'Bloomsbury', 'Quercus', 'Random House', 'Random House', 'Pan Macmillan', 'Penguin', 'Random House', 'Random House', 'Canongate', 'HarperCollins', 'Orion', 'Pan Macmillan', 'HarperCollins', 'Random House', 'Scholastic Ltd.', 'Orion', 'Bloomsbury', 'Penguin', 'HarperCollins', 'Random House', 'Little, Brown Book', 'Headline', 'HarperCollins', 'Penguin', 'Orion', 'Bloomsbury', 'Random House', 'Random House', 'Scholastic Ltd.', 'Profile Books Group', 'Random House', 'Transworld', 'Random House Childrens Books G', 'Hodder & Stoughton', 'Scholastic Ltd.', 'Random House', 'HarperCollins', 'Random House', 'Penguin', 'Random House', 'HarperCollins', 'Penguin', 'Headline', 'Little, Brown Book', 'HarperCollins', 'Transworld', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Transworld', 'Transworld', 'Random House', 'Headline', 'Random House', 'Transworld', 'Penguin', 'Pan Macmillan', 'Penguin', 'Hodder & Stoughton', 'Transworld', 'D.C. Thomson', 'Penguin', 'Pan Macmillan', 'Transworld', 'Orion', 'Random House', 'Penguin', 'Scholastic Ltd.', 'Orion', 'Penguin']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Publisher\n",
    "\n",
    "Publisher=[]\n",
    "\n",
    "publisher_tags=driver.find_elements(By.XPATH,'//tr/td[@class=\"left\"][5]')\n",
    "for i in publisher_tags:\n",
    "    title=i.text\n",
    "    Publisher.append(title)\n",
    "\n",
    "print(len(Publisher),Publisher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "360fd53e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Crime, Thriller & Adventure', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Romance & Sagas', \"Children's Fiction\", \"Children's Fiction\", \"Children's Fiction\", 'Crime, Thriller & Adventure', \"Children's Fiction\", 'Romance & Sagas', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Romance & Sagas', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Popular Science', 'Crime, Thriller & Adventure', 'Young Adult Fiction', 'Picture Books', 'Picture Books', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Humour: Collections & General', 'General & Literary Fiction', 'Autobiography: General', 'General & Literary Fiction', 'Biography: The Arts', 'Autobiography: General', 'Picture Books', 'Autobiography: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Science Fiction & Fantasy', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'General & Literary Fiction', 'Fitness & Diet', 'General & Literary Fiction', \"Children's Fiction\", 'Crime, Thriller & Adventure', 'Fitness & Diet', 'Young Adult Fiction', 'Usage & Writing Guides', 'Food & Drink: General', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Young Adult Fiction', 'General & Literary Fiction', 'Popular Culture & Media: General Interest', 'Autobiography: The Arts', 'General & Literary Fiction', 'Crime, Thriller & Adventure', 'Science Fiction & Fantasy', 'Current Affairs & Issues', 'Crime, Thriller & Adventure', 'Autobiography: The Arts', 'General & Literary Fiction', 'Autobiography: The Arts', 'Travel Writing', 'Food & Drink: General', 'General & Literary Fiction', 'National & Regional Cuisine', 'Fitness & Diet', 'Travel Writing', 'Crime, Thriller & Adventure', 'General & Literary Fiction', 'Food & Drink: General', 'General & Literary Fiction', 'General & Literary Fiction', 'Picture Books', 'General & Literary Fiction', 'Autobiography: The Arts', 'Popular Science', \"Children's Annuals\", 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'General & Literary Fiction', 'Food & Drink: General', 'Young Adult Fiction', 'Biography: General', 'Food & Drink: General']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Publisher\n",
    "\n",
    "Genres=[]\n",
    "\n",
    "genre_tags=driver.find_elements(By.XPATH,'//td[@class=\"last left\"]')\n",
    "for i in genre_tags:\n",
    "    title=i.text\n",
    "    Genres.append(title)\n",
    "\n",
    "print(len(Genres),Genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ef6e247d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Books Name</th>\n",
       "      <th>Author Name</th>\n",
       "      <th>Volume Sold</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Da Vinci Code,The</td>\n",
       "      <td>Brown, Dan</td>\n",
       "      <td>5,094,805</td>\n",
       "      <td>Transworld</td>\n",
       "      <td>Crime, Thriller &amp; Adventure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Harry Potter and the Deathly Hallows</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,475,152</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,200,654</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Harry Potter and the Order of the Phoenix</td>\n",
       "      <td>Rowling, J.K.</td>\n",
       "      <td>4,179,479</td>\n",
       "      <td>Bloomsbury</td>\n",
       "      <td>Children's Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fifty Shades of Grey</td>\n",
       "      <td>James, E. L.</td>\n",
       "      <td>3,758,936</td>\n",
       "      <td>Random House</td>\n",
       "      <td>Romance &amp; Sagas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Ghost,The</td>\n",
       "      <td>Harris, Robert</td>\n",
       "      <td>807,311</td>\n",
       "      <td>Random House</td>\n",
       "      <td>General &amp; Literary Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Happy Days with the Naked Chef</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>794,201</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hunger Games,The:Hunger Games Trilogy</td>\n",
       "      <td>Collins, Suzanne</td>\n",
       "      <td>792,187</td>\n",
       "      <td>Scholastic Ltd.</td>\n",
       "      <td>Young Adult Fiction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Lost Boy,The:A Foster Child's Search for the L...</td>\n",
       "      <td>Pelzer, Dave</td>\n",
       "      <td>791,507</td>\n",
       "      <td>Orion</td>\n",
       "      <td>Biography: General</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Jamie's Ministry of Food:Anyone Can Learn to C...</td>\n",
       "      <td>Oliver, Jamie</td>\n",
       "      <td>791,095</td>\n",
       "      <td>Penguin</td>\n",
       "      <td>Food &amp; Drink: General</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Books Name       Author Name  \\\n",
       "0                                   Da Vinci Code,The        Brown, Dan   \n",
       "1                Harry Potter and the Deathly Hallows     Rowling, J.K.   \n",
       "2            Harry Potter and the Philosopher's Stone     Rowling, J.K.   \n",
       "3           Harry Potter and the Order of the Phoenix     Rowling, J.K.   \n",
       "4                                Fifty Shades of Grey      James, E. L.   \n",
       "..                                                ...               ...   \n",
       "95                                          Ghost,The    Harris, Robert   \n",
       "96                     Happy Days with the Naked Chef     Oliver, Jamie   \n",
       "97              Hunger Games,The:Hunger Games Trilogy  Collins, Suzanne   \n",
       "98  Lost Boy,The:A Foster Child's Search for the L...      Pelzer, Dave   \n",
       "99  Jamie's Ministry of Food:Anyone Can Learn to C...     Oliver, Jamie   \n",
       "\n",
       "   Volume Sold        Publisher                        Genre  \n",
       "0    5,094,805       Transworld  Crime, Thriller & Adventure  \n",
       "1    4,475,152       Bloomsbury           Children's Fiction  \n",
       "2    4,200,654       Bloomsbury           Children's Fiction  \n",
       "3    4,179,479       Bloomsbury           Children's Fiction  \n",
       "4    3,758,936     Random House              Romance & Sagas  \n",
       "..         ...              ...                          ...  \n",
       "95     807,311     Random House   General & Literary Fiction  \n",
       "96     794,201          Penguin        Food & Drink: General  \n",
       "97     792,187  Scholastic Ltd.          Young Adult Fiction  \n",
       "98     791,507            Orion           Biography: General  \n",
       "99     791,095          Penguin        Food & Drink: General  \n",
       "\n",
       "[100 rows x 5 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Novels=pd.DataFrame({'Books Name':Books_name,'Author Name': Author_name,'Volume Sold':Volume_Sold,'Publisher':Publisher,'Genre':Genres})\n",
    "Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4486e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784455c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18c3998b",
   "metadata": {},
   "source": [
    "<a id=\"imdb\"> </a>\n",
    "## 7. Scrape the details most watched tv series of all time from imdb.com.  Url = https://www.imdb.com/list/ls095964455/.\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "    A) Name\n",
    "    B) Year span\n",
    "    C) Genre\n",
    "    D) Run time\n",
    "    E) Ratings\n",
    "    F) Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "68c40616",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89e5c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the imdb page on automated chrome browser\n",
    "\n",
    "driver.get('https://www.imdb.com/list/ls095964455/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5014f855",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['1. Game of Thrones (2011–2019)', '2. Stranger Things (2016–2022)', '3. The Walking Dead (2010–2022)', '4. 13 Reasons Why (2017–2020)', '5. The 100 (2014–2020)', '6. Orange Is the New Black (2013–2019)', '7. Riverdale (2017–2023)', \"8. Grey's Anatomy (2005– )\", '9. The Flash (2014–2023)', '10. Arrow (2012–2020)', '11. Money Heist (2017–2021)', '12. The Big Bang Theory (2007–2019)', '13. Black Mirror (2011– )', '14. Sherlock (2010–2017)', '15. Vikings (2013–2020)', '16. Pretty Little Liars (2010–2017)', '17. The Vampire Diaries (2009–2017)', '18. American Horror Story (2011– )', '19. Breaking Bad (2008–2013)', '20. Lucifer (2016–2021)', '21. Supernatural (2005–2020)', '22. Prison Break (2005–2017)', '23. How to Get Away with Murder (2014–2020)', '24. Teen Wolf (2011–2017)', '25. The Simpsons (1989– )', '26. Once Upon a Time (2011–2018)', '27. Narcos (2015–2017)', '28. Daredevil (2015–2018)', '29. Friends (1994–2004)', '30. How I Met Your Mother (2005–2014)', '31. Suits (2011–2019)', '32. Mr. Robot (2015–2019)', '33. The Originals (2013–2018)', '34. Supergirl (2015–2021)', '35. Gossip Girl (2007–2012)', '36. Sense8 (2015–2018)', '37. Gotham (2014–2019)', '38. Westworld (2016–2022)', '39. Jessica Jones (2015–2019)', '40. Modern Family (2009–2020)', '41. Rick and Morty (2013– )', '42. Shadowhunters (2016–2019)', '43. The End of the F***ing World (2017–2019)', '44. House of Cards (2013–2018)', '45. Dark (2017–2020)', '46. Elite (2018– )', '47. Sex Education (2019– )', '48. Shameless (2011–2021)', '49. New Girl (2011–2018)', '50. Agents of S.H.I.E.L.D. (2013–2020)', '51. You (2018–2024)', '52. Dexter (2006–2013)', '53. Fear the Walking Dead (2015–2023)', '54. Family Guy (1999– )', '55. The Blacklist (2013–2023)', '56. Lost (2004–2010)', '57. Peaky Blinders (2013–2022)', '58. House (2004–2012)', '59. Quantico (2015–2018)', '60. Orphan Black (2013–2017)', '61. Homeland (2011–2020)', '62. Blindspot (2015–2020)', \"63. DC's Legends of Tomorrow (2016–2022)\", \"64. The Handmaid's Tale (2017– )\", '65. Chilling Adventures of Sabrina (2018–2020)', '66. The Good Doctor (2017– )', '67. Jane the Virgin (2014–2019)', '68. Glee (2009–2015)', '69. South Park (1997– )', '70. Brooklyn Nine-Nine (2013–2021)', '71. Under the Dome (2013–2015)', '72. The Umbrella Academy (2019–2023)', '73. True Detective (2014– )', '74. The OA (2016–2019)', '75. Desperate Housewives (2004–2012)', '76. Better Call Saul (2015–2022)', '77. Bates Motel (2013–2017)', '78. The Punisher (2017–2019)', '79. Atypical (2017–2021)', '80. Dynasty (2017–2022)', '81. This Is Us (2016–2022)', '82. The Good Place (2016–2020)', '83. Iron Fist (2017–2018)', '84. The Rain (2018–2020)', '85. Mindhunter (2017–2019)', '86. Revenge (2011–2015)', '87. Luke Cage (2016–2018)', '88. Scandal (2012–2018)', '89. The Defenders (2017)', '90. Big Little Lies (2017–2019)', '91. Insatiable (2018–2019)', '92. The Mentalist (2008–2015)', '93. The Crown (2016–2023)', '94. Chernobyl (2019)', '95. iZombie (2015–2019)', '96. Reign (2013–2017)', '97. A Series of Unfortunate Events (2017–2019)', '98. Criminal Minds (2005– )', '99. Scream: The TV Series (2015–2019)', '100. The Haunting of Hill House (2018)']\n"
     ]
    }
   ],
   "source": [
    "#scraping the TV series Name \n",
    "\n",
    "TV_series_name=[]\n",
    "\n",
    "tv_series_name=driver.find_elements(By.XPATH,'//h3[@class=\"lister-item-header\"]')\n",
    "for i in tv_series_name:\n",
    "    title=i.text\n",
    "    TV_series_name.append(title)\n",
    "\n",
    "print(len(TV_series_name),TV_series_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "eff4d347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['(2011–2019)', '(2016–2022)', '(2010–2022)', '(2017–2020)', '(2014–2020)', '(2013–2019)', '(2017–2023)', '(2005– )', '(2014–2023)', '(2012–2020)', '(2017–2021)', '(2007–2019)', '(2011– )', '(2010–2017)', '(2013–2020)', '(2010–2017)', '(2009–2017)', '(2011– )', '(2008–2013)', '(2016–2021)', '(2005–2020)', '(2005–2017)', '(2014–2020)', '(2011–2017)', '(1989– )', '(2011–2018)', '(2015–2017)', '(2015–2018)', '(1994–2004)', '(2005–2014)', '(2011–2019)', '(2015–2019)', '(2013–2018)', '(2015–2021)', '(2007–2012)', '(2015–2018)', '(2014–2019)', '(2016–2022)', '(2015–2019)', '(2009–2020)', '(2013– )', '(2016–2019)', '(2017–2019)', '(2013–2018)', '(2017–2020)', '(2018– )', '(2019– )', '(2011–2021)', '(2011–2018)', '(2013–2020)', '(2018–2024)', '(2006–2013)', '(2015–2023)', '(1999– )', '(2013–2023)', '(2004–2010)', '(2013–2022)', '(2004–2012)', '(2015–2018)', '(2013–2017)', '(2011–2020)', '(2015–2020)', '(2016–2022)', '(2017– )', '(2018–2020)', '(2017– )', '(2014–2019)', '(2009–2015)', '(1997– )', '(2013–2021)', '(2013–2015)', '(2019–2023)', '(2014– )', '(2016–2019)', '(2004–2012)', '(2015–2022)', '(2013–2017)', '(2017–2019)', '(2017–2021)', '(2017–2022)', '(2016–2022)', '(2016–2020)', '(2017–2018)', '(2018–2020)', '(2017–2019)', '(2011–2015)', '(2016–2018)', '(2012–2018)', '(2017)', '(2017–2019)', '(2018–2019)', '(2008–2015)', '(2016–2023)', '(2019)', '(2015–2019)', '(2013–2017)', '(2017–2019)', '(2005– )', '(2015–2019)', '(2018)']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Year_span \n",
    "\n",
    "Year_span=[]\n",
    "\n",
    "year_span=driver.find_elements(By.XPATH,\"//span[@class='lister-item-year text-muted unbold']\")\n",
    "for i in year_span:\n",
    "    title=i.text\n",
    "    Year_span.append(title)\n",
    "\n",
    "print(len(Year_span),Year_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "77cd9316",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['Action, Adventure, Drama', 'Drama, Fantasy, Horror', 'Drama, Horror, Thriller', 'Drama, Mystery, Thriller', 'Drama, Mystery, Sci-Fi', 'Comedy, Crime, Drama', 'Crime, Drama, Mystery', 'Drama, Romance', 'Action, Adventure, Drama', 'Action, Adventure, Crime', 'Action, Crime, Drama', 'Comedy, Romance', 'Drama, Mystery, Sci-Fi', 'Crime, Drama, Mystery', 'Action, Adventure, Drama', 'Drama, Mystery, Romance', 'Drama, Fantasy, Horror', 'Drama, Horror, Sci-Fi', 'Crime, Drama, Thriller', 'Crime, Drama, Fantasy', 'Drama, Fantasy, Horror', 'Action, Crime, Drama', 'Crime, Drama, Mystery', 'Action, Drama, Fantasy', 'Animation, Comedy', 'Adventure, Fantasy, Romance', 'Biography, Crime, Drama', 'Action, Crime, Drama', 'Comedy, Romance', 'Comedy, Drama, Romance', 'Comedy, Drama', 'Crime, Drama, Thriller', 'Drama, Fantasy, Horror', 'Action, Adventure, Drama', 'Drama, Romance', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Drama, Mystery, Sci-Fi', 'Action, Crime, Drama', 'Comedy, Drama, Romance', 'Animation, Adventure, Comedy', 'Action, Drama, Fantasy', 'Adventure, Comedy, Crime', 'Drama', 'Crime, Drama, Mystery', 'Crime, Drama, Thriller', 'Comedy, Drama', 'Comedy, Drama', 'Comedy, Romance', 'Action, Adventure, Drama', 'Crime, Drama, Romance', 'Crime, Drama, Mystery', 'Drama, Horror, Sci-Fi', 'Animation, Comedy', 'Crime, Drama, Mystery', 'Adventure, Drama, Fantasy', 'Crime, Drama', 'Drama, Mystery', 'Crime, Drama, Mystery', 'Drama, Sci-Fi, Thriller', 'Crime, Drama, Mystery', 'Action, Crime, Drama', 'Action, Adventure, Drama', 'Drama, Sci-Fi, Thriller', 'Drama, Fantasy, Horror', 'Drama', 'Comedy', 'Comedy, Drama, Music', 'Animation, Comedy', 'Comedy, Crime', 'Drama, Mystery, Sci-Fi', 'Action, Adventure, Comedy', 'Crime, Drama, Mystery', 'Drama, Fantasy, Mystery', 'Comedy, Drama, Mystery', 'Crime, Drama', 'Drama, Horror, Mystery', 'Action, Crime, Drama', 'Comedy, Drama', 'Drama', 'Comedy, Drama, Romance', 'Comedy, Drama, Fantasy', 'Action, Adventure, Crime', 'Drama, Sci-Fi, Thriller', 'Crime, Drama, Mystery', 'Drama, Mystery, Thriller', 'Action, Crime, Drama', 'Drama, Thriller', 'Action, Adventure, Crime', 'Crime, Drama, Mystery', 'Comedy, Drama, Thriller', 'Crime, Drama, Mystery', 'Biography, Drama, History', 'Drama, History, Thriller', 'Comedy, Crime, Drama', 'Drama', 'Adventure, Comedy, Drama', 'Crime, Drama, Mystery', 'Comedy, Crime, Drama', 'Drama, Horror, Mystery']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Genres\n",
    "\n",
    "Genres=[]\n",
    "\n",
    "genre=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small']/span[5]\")\n",
    "for i in genre:\n",
    "    title=i.text\n",
    "    Genres.append(title)\n",
    "\n",
    "print(len(Genres),Genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29f5dca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['57 min', '51 min', '44 min', '60 min', '43 min', '59 min', '45 min', '41 min', '43 min', '42 min', '70 min', '22 min', '60 min', '88 min', '44 min', '44 min', '43 min', '60 min', '49 min', '42 min', '44 min', '44 min', '43 min', '41 min', '22 min', '60 min', '49 min', '54 min', '22 min', '22 min', '44 min', '49 min', '45 min', '43 min', '42 min', '60 min', '42 min', '62 min', '56 min', '22 min', '23 min', '42 min', '25 min', '51 min', '60 min', '60 min', '45 min', '46 min', '22 min', '45 min', '45 min', '53 min', '44 min', '22 min', '43 min', '44 min', '60 min', '44 min', '42 min', '44 min', '55 min', '42 min', '42 min', '60 min', '60 min', '41 min', '60 min', '44 min', '22 min', '22 min', '43 min', '60 min', '55 min', '60 min', '45 min', '46 min', '45 min', '53 min', '30 min', '42 min', '45 min', '22 min', '55 min', '45 min', '60 min', '44 min', '55 min', '43 min', '50 min', '60 min', '45 min', '43 min', '58 min', '330 min', '42 min', '42 min', '50 min', '42 min', '45 min', '572 min']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Run_time \n",
    "\n",
    "Run_Time=[]\n",
    "\n",
    "run_time=driver.find_elements(By.XPATH,\"//p[@class='text-muted text-small']/span[3]\")\n",
    "for i in run_time:\n",
    "    title=i.text\n",
    "    Run_Time.append(title)\n",
    "\n",
    "print(len(Run_Time),Run_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c37ceb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['9.2', '8.7', '8.1', '7.5', '7.6', '8.1', '6.5', '7.6', '7.5', '7.5', '8.2', '8.2', '8.8', '9.1', '8.5', '7.4', '7.7', '8', '9.5', '8.1', '8.4', '8.3', '8.1', '7.7', '8.7', '7.7', '8.8', '8.6', '8.9', '8.3', '8.4', '8.5', '8.3', '6.2', '7.5', '8.2', '7.8', '8.5', '7.9', '8.5', '9.1', '6.5', '8', '8.7', '8.7', '7.3', '8.3', '8.6', '7.8', '7.5', '7.7', '8.7', '6.8', '8.2', '8', '8.3', '8.8', '8.7', '6.7', '8.3', '8.3', '7.3', '6.7', '8.4', '7.4', '8', '7.9', '6.8', '8.7', '8.4', '6.5', '7.9', '8.9', '7.8', '7.6', '8.9', '8.1', '8.5', '8.2', '7.2', '8.7', '8.2', '6.4', '6.3', '8.6', '7.8', '7.3', '7.7', '7.2', '8.5', '6.5', '8.1', '8.6', '9.4', '7.8', '7.4', '7.8', '8.1', '7', '8.6']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Ratings \n",
    "\n",
    "Ratings=[]\n",
    "\n",
    "rating=driver.find_elements(By.XPATH,\"//div[@class='ipl-rating-star small']/span[2]\")\n",
    "for i in rating:\n",
    "    title=i.text\n",
    "    Ratings.append(title)\n",
    "\n",
    "print(len(Ratings),Ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cba23a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 ['2,162,642', '1,242,679', '1,027,393', '302,221', '261,324', '309,622', '148,865', '321,735', '356,677', '437,645', '495,844', '828,884', '572,699', '950,409', '551,459', '172,009', '331,306', '327,194', '1,977,147', '336,117', '458,794', '551,692', '157,559', '155,769', '417,725', '229,418', '441,825', '453,018', '1,026,074', '700,808', '425,325', '398,554', '140,715', '126,575', '180,974', '157,938', '234,907', '515,599', '219,438', '450,196', '551,462', '66,578', '202,649', '514,568', '408,680', '84,105', '299,865', '254,714', '233,484', '220,665', '278,595', '738,585', '134,791', '350,453', '262,062', '567,763', '581,035', '479,271', '62,268', '113,360', '349,457', '76,285', '107,336', '246,760', '100,750', '102,433', '54,235', '151,560', '388,110', '332,680', '109,065', '257,947', '594,671', '108,477', '132,930', '575,171', '111,681', '247,444', '95,527', '23,703', '149,564', '171,697', '134,837', '39,167', '305,599', '122,106', '134,864', '76,562', '111,659', '209,850', '30,506', '190,953', '230,098', '796,912', '70,963', '51,685', '63,720', '207,688', '43,217', '258,221']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Votes\n",
    "\n",
    "Votes=[]\n",
    "\n",
    "votes=driver.find_elements(By.XPATH,\"//div[@class='lister-item-content']/p[4]/span[2]\")\n",
    "for i in votes:\n",
    "    title=i.text\n",
    "    Votes.append(title)\n",
    "\n",
    "print(len(Votes),Votes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24074546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TV series name</th>\n",
       "      <th>Year span</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Run Time</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Game of Thrones (2011–2019)</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.2</td>\n",
       "      <td>2,162,642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2. Stranger Things (2016–2022)</td>\n",
       "      <td>(2016–2022)</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1,242,679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3. The Walking Dead (2010–2022)</td>\n",
       "      <td>(2010–2022)</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>1,027,393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4. 13 Reasons Why (2017–2020)</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>302,221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5. The 100 (2014–2020)</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>261,324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96. Reign (2013–2017)</td>\n",
       "      <td>(2013–2017)</td>\n",
       "      <td>Drama</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.4</td>\n",
       "      <td>51,685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97. A Series of Unfortunate Events (2017–2019)</td>\n",
       "      <td>(2017–2019)</td>\n",
       "      <td>Adventure, Comedy, Drama</td>\n",
       "      <td>50 min</td>\n",
       "      <td>7.8</td>\n",
       "      <td>63,720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98. Criminal Minds (2005– )</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>42 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>207,688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99. Scream: The TV Series (2015–2019)</td>\n",
       "      <td>(2015–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>45 min</td>\n",
       "      <td>7</td>\n",
       "      <td>43,217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100. The Haunting of Hill House (2018)</td>\n",
       "      <td>(2018)</td>\n",
       "      <td>Drama, Horror, Mystery</td>\n",
       "      <td>572 min</td>\n",
       "      <td>8.6</td>\n",
       "      <td>258,221</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    TV series name    Year span  \\\n",
       "0                   1. Game of Thrones (2011–2019)  (2011–2019)   \n",
       "1                   2. Stranger Things (2016–2022)  (2016–2022)   \n",
       "2                  3. The Walking Dead (2010–2022)  (2010–2022)   \n",
       "3                    4. 13 Reasons Why (2017–2020)  (2017–2020)   \n",
       "4                           5. The 100 (2014–2020)  (2014–2020)   \n",
       "..                                             ...          ...   \n",
       "95                           96. Reign (2013–2017)  (2013–2017)   \n",
       "96  97. A Series of Unfortunate Events (2017–2019)  (2017–2019)   \n",
       "97                     98. Criminal Minds (2005– )     (2005– )   \n",
       "98           99. Scream: The TV Series (2015–2019)  (2015–2019)   \n",
       "99          100. The Haunting of Hill House (2018)       (2018)   \n",
       "\n",
       "                       Genre Run Time Ratings      Votes  \n",
       "0   Action, Adventure, Drama   57 min     9.2  2,162,642  \n",
       "1     Drama, Fantasy, Horror   51 min     8.7  1,242,679  \n",
       "2    Drama, Horror, Thriller   44 min     8.1  1,027,393  \n",
       "3   Drama, Mystery, Thriller   60 min     7.5    302,221  \n",
       "4     Drama, Mystery, Sci-Fi   43 min     7.6    261,324  \n",
       "..                       ...      ...     ...        ...  \n",
       "95                     Drama   42 min     7.4     51,685  \n",
       "96  Adventure, Comedy, Drama   50 min     7.8     63,720  \n",
       "97     Crime, Drama, Mystery   42 min     8.1    207,688  \n",
       "98      Comedy, Crime, Drama   45 min       7     43,217  \n",
       "99    Drama, Horror, Mystery  572 min     8.6    258,221  \n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "TV_Series=pd.DataFrame({'TV series name':TV_series_name,'Year span': Year_span,'Genre':Genres,'Run Time':Run_Time,'Ratings':Ratings,'Votes':Votes})\n",
    "TV_Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1516057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d06e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc6fdcf0",
   "metadata": {},
   "source": [
    "<a id=\"repositories\"> </a>\n",
    "## 8. Details of Datasetsfrom UCI machine learning repositories.  Url = https://archive.ics.uci.edu/.\n",
    "\n",
    "You have to find the following details:\n",
    "\n",
    "    A) Dataset name\n",
    "    B) Data type\n",
    "    C) Task\n",
    "    D) Attribute type\n",
    "    E) No of instances\n",
    "    F) No of attribute\n",
    "    G) Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb3db1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0d8b1e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the amazon page on automated chrome browser\n",
    "\n",
    "driver.get('https://archive.ics.uci.edu/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b4c1f544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating page for view all datasets by xpath\n",
    "\n",
    "search = driver.find_element(By.XPATH,'/html/body/table[1]/tbody/tr/td[2]/span[2]/a')\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3d76f5c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ['Abalone', 'Adult', 'Annealing', 'Anonymous Microsoft Web Data', 'Arrhythmia', 'Artificial Characters', 'Audiology (Original)', 'Audiology (Standardized)', 'Auto MPG', 'Automobile', 'Badges', 'Balance Scale', 'Balloons', 'Breast Cancer', 'Breast Cancer Wisconsin (Original)', 'Breast Cancer Wisconsin (Prognostic)', 'Breast Cancer Wisconsin (Diagnostic)', 'Pittsburgh Bridges', 'Car Evaluation', 'Census Income', 'Chess (King-Rook vs. King-Knight)', 'Chess (King-Rook vs. King-Pawn)', 'Chess (King-Rook vs. King)', 'Chess (Domain Theories)', 'Bach Chorales', 'Connect-4', 'Credit Approval', 'Japanese Credit Screening', 'Computer Hardware', 'Contraceptive Method Choice', 'Covertype', 'Cylinder Bands', 'Dermatology', 'Diabetes', 'DGP2 - The Second Data Generation Program', 'Document Understanding', 'EBL Domain Theories', 'Echocardiogram', 'Ecoli', 'Flags', 'Function Finding', 'Glass Identification', \"Haberman's Survival\", 'Hayes-Roth', 'Heart Disease', 'Hepatitis', 'Horse Colic', 'ICU', 'Image Segmentation', 'Internet Advertisements', 'Ionosphere', 'Iris', 'ISOLET', 'Kinship', 'Labor Relations', 'LED Display Domain', 'Lenses', 'Letter Recognition', 'Liver Disorders', 'Logic Theorist', 'Lung Cancer', 'Lymphography', 'Mechanical Analysis', 'Meta-data', 'Mobile Robots', 'Molecular Biology (Promoter Gene Sequences)', 'Molecular Biology (Protein Secondary Structure)', 'Molecular Biology (Splice-junction Gene Sequences)', \"MONK's Problems\", 'Moral Reasoner', 'Multiple Features', 'Mushroom', 'Musk (Version 1)', 'Musk (Version 2)', 'Nursery', 'Othello Domain Theory', 'Page Blocks Classification', 'Optical Recognition of Handwritten Digits', 'Pen-Based Recognition of Handwritten Digits', 'Post-Operative Patient', 'Primary Tumor', 'Prodigy', 'Qualitative Structure Activity Relationships', 'Quadruped Mammals', 'Servo', 'Shuttle Landing Control', 'Solar Flare', 'Soybean (Large)', 'Soybean (Small)', 'Challenger USA Space Shuttle O-Ring', 'Low Resolution Spectrometer', 'Spambase', 'SPECT Heart', 'SPECTF Heart', 'Sponge', 'Statlog Project', 'Student Loan Relational', 'Teaching Assistant Evaluation', 'Tic-Tac-Toe Endgame', 'Thyroid Disease', 'Trains', 'University', 'Congressional Voting Records', 'Water Treatment Plant', 'Waveform Database Generator (Version 1)', 'Waveform Database Generator (Version 2)', 'Wine', 'Yeast', 'Zoo', 'Undocumented', 'Twenty Newsgroups', 'Australian Sign Language signs', 'Australian Sign Language signs (High Quality)', 'US Census Data (1990)', 'Census-Income (KDD)', 'Coil 1999 Competition Data', 'Corel Image Features', 'E. Coli Genes', 'EEG Database', 'El Nino', 'Entree Chicago Recommendation Data', 'CMU Face Images', 'Insurance Company Benchmark (COIL 2000)', 'Internet Usage Data', 'IPUMS Census Database', 'Japanese Vowels', 'KDD Cup 1998 Data', 'KDD Cup 1999 Data', 'M. Tuberculosis Genes', 'Movie', 'MSNBC.com Anonymous Web Data', 'NSF Research Award Abstracts 1990-2003', 'Pioneer-1 Mobile Robot Data', 'Pseudo Periodic Synthetic Time Series', 'Reuters-21578 Text Categorization Collection', 'Robot Execution Failures', 'Synthetic Control Chart Time Series', 'Syskill and Webert Web Page Ratings', 'UNIX User Data', 'Volcanoes on Venus - JARtool experiment', 'Statlog (Australian Credit Approval)', 'Statlog (German Credit Data)', 'Statlog (Heart)', 'Statlog (Landsat Satellite)', 'Statlog (Image Segmentation)', 'Statlog (Shuttle)', 'Statlog (Vehicle Silhouettes)', 'Connectionist Bench (Nettalk Corpus)', 'Connectionist Bench (Sonar, Mines vs. Rocks)', 'Connectionist Bench (Vowel Recognition - Deterding Data)', 'Economic Sanctions', 'Protein Data', 'Cloud', 'CalIt2 Building People Counts', 'Dodgers Loop Sensor', 'Poker Hand', 'MAGIC Gamma Telescope', 'UJI Pen Characters', 'Mammographic Mass', 'Forest Fires', 'Reuters Transcribed Subset', 'Bag of Words', 'Concrete Compressive Strength', 'Hill-Valley', 'Arcene', 'Dexter', 'Dorothea', 'Gisette', 'Madelon', 'Ozone Level Detection', 'Abscisic Acid Signaling Network', 'Parkinsons', 'Character Trajectories', 'Blood Transfusion Service Center', 'UJI Pen Characters (Version 2)', 'Semeion Handwritten Digit', 'SECOM', 'Plants', 'Libras Movement', 'Concrete Slump Test', 'Communities and Crime', 'Acute Inflammations', 'Wine Quality', 'URL Reputation', 'p53 Mutants', 'Parkinsons Telemonitoring', 'Demospongiae', 'Opinosis Opinion ⁄ Review', 'Breast Tissue', 'Cardiotocography', 'Wall-Following Robot Navigation Data', 'Spoken Arabic Digit', 'Localization Data for Person Activity', 'AutoUniv', 'Steel Plates Faults', 'MiniBooNE particle identification', 'YearPredictionMSD', 'PEMS-SF', 'OpinRank Review Dataset', 'Relative location of CT slices on axial axis', 'Online Handwritten Assamese Characters Dataset', 'PubChem Bioassay Data', 'Record Linkage Comparison Patterns', 'Communities and Crime Unnormalized', 'Vertebral Column', 'EMG Physical Action Data Set', 'Vicon Physical Action Data Set', 'Amazon Commerce reviews set', 'Amazon Access Samples', 'Reuter_50_50', 'Farm Ads', 'DBWorld e-mails', 'KEGG Metabolic Relation Network (Directed)', 'KEGG Metabolic Reaction Network (Undirected)', 'Bank Marketing', 'YouTube Comedy Slam Preference Data', 'Gas Sensor Array Drift Dataset', 'ILPD (Indian Liver Patient Dataset)', 'OPPORTUNITY Activity Recognition', 'Nomao', 'SMS Spam Collection', 'Skin Segmentation', 'Planning Relax', 'PAMAP2 Physical Activity Monitoring', 'Restaurant & consumer data', 'CNAE-9', 'Individual household electric power consumption', 'seeds', 'Northix', 'QtyT40I10D100K', 'Legal Case Reports', 'Human Activity Recognition Using Smartphones', 'One-hundred plant species leaves data set', 'Energy efficiency', 'Yacht Hydrodynamics', 'Fertility', 'Daphnet Freezing of Gait', '3D Road Network (North Jutland, Denmark)', 'ISTANBUL STOCK EXCHANGE', 'Buzz in social media', 'First-order theorem proving', 'Wearable Computing: Classification of Body Postures and Movements (PUC-Rio)', 'Gas sensor arrays in open sampling settings', 'Climate Model Simulation Crashes', 'MicroMass', 'QSAR biodegradation', 'BLOGGER', 'Daily and Sports Activities', 'User Knowledge Modeling', 'Reuters RCV1 RCV2 Multilingual, Multiview Text Categorization Test collection', 'NYSK', 'Turkiye Student Evaluation', \"ser Knowledge Modeling Data (Students' Knowledge Levels on DC Electrical Machines)\", 'EEG Eye State', 'Physicochemical Properties of Protein Tertiary Structure', 'seismic-bumps', 'banknote authentication', 'USPTO Algorithm Challenge, run by NASA-Harvard Tournament Lab and TopCoder Problem: Pat', 'YouTube Multiview Video Games Dataset', 'Gas Sensor Array Drift Dataset at Different Concentrations', 'Activities of Daily Living (ADLs) Recognition Using Binary Sensors', 'SkillCraft1 Master Table Dataset', 'Weight Lifting Exercises monitored with Inertial Measurement Units', 'SML2010', 'Bike Sharing Dataset', 'Predict keywords activities in a online social media', 'Thoracic Surgery Data', 'EMG dataset in Lower Limb', 'SUSY', 'HIGGS', 'Qualitative_Bankruptcy', 'LSVT Voice Rehabilitation', 'Dataset for ADL Recognition with Wrist-worn Accelerometer', 'Wilt', 'User Identification From Walking Activity', 'Activity Recognition from Single Chest-Mounted Accelerometer', 'Leaf', 'Dresses_Attribute_Sales', 'Tamilnadu Electricity Board Hourly Readings', 'Airfoil Self-Noise', 'Wholesale customers', 'Twitter Data set for Arabic Sentiment Analysis', 'Combined Cycle Power Plant', 'Urban Land Cover', 'Diabetes 130-US hospitals for years 1999-2008', 'Bach Choral Harmony', 'StoneFlakes', 'Tennis Major Tournament Match Statistics', 'Parkinson Speech Dataset with Multiple Types of Sound Recordings', 'Gesture Phase Segmentation', 'Perfume Data', 'BlogFeedback', 'REALDISP Activity Recognition Dataset', 'Newspaper and magazine images segmentation dataset', 'AAAI 2014 Accepted Papers', 'Gas sensor array under flow modulation', 'Gas sensor array exposed to turbulent gas mixtures', 'UJIIndoorLoc', 'Sentence Classification', 'Dow Jones Index', 'sEMG for Basic Hand movements', 'AAAI 2013 Accepted Papers', 'Geographical Original of Music', 'Condition Based Maintenance of Naval Propulsion Plants', 'Grammatical Facial Expressions', 'NoisyOffice', 'MHEALTH Dataset', 'Student Performance', 'ElectricityLoadDiagrams20112014', 'Gas sensor array under dynamic gas mixtures', 'microblogPCU', 'Firm-Teacher_Clave-Direction_Classification', 'Dataset for Sensorless Drive Diagnosis', 'TV News Channel Commercial Detection Dataset', 'Phishing Websites', 'Greenhouse Gas Observing Network', 'Diabetic Retinopathy Debrecen Data Set', 'HIV-1 protease cleavage', 'Sentiment Labelled Sentences', 'Online News Popularity', 'Forest type mapping', 'wiki4HE', 'Online Video Characteristics and Transcoding Time Dataset', 'Chronic_Kidney_Disease', 'Machine Learning based ZZAlpha Ltd. Stock Recommendations 2012-2014', 'Folio', 'Taxi Service Trajectory - Prediction Challenge, ECML PKDD 2015', 'Cuff-Less Blood Pressure Estimation', 'Smartphone-Based Recognition of Human Activities and Postural Transitions', 'Mice Protein Expression', 'UJIIndoorLoc-Mag', 'Heterogeneity Activity Recognition', 'Educational Process Mining (EPM): A Learning Analytics Data Set', 'HEPMASS', 'Indoor User Movement Prediction from RSS data', 'Open University Learning Analytics dataset', 'default of credit card clients', 'Mesothelioma’s disease data set', 'Online Retail', 'SIFT10M', 'GPS Trajectories', 'Detect Malacious Executable(AntiVirus)', 'Occupancy Detection', 'Improved Spiral Test Using Digitized Graphics Tablet for Monitoring Parkinson’s Disease', 'News Aggregator', 'Air Quality', 'Twin gas sensor arrays', 'Gas sensors for home activity monitoring', 'Facebook Comment Volume Dataset', 'Smartphone Dataset for Human Activity Recognition (HAR) in Ambient Assisted Living (AAL)', 'Polish companies bankruptcy data', 'Activity Recognition system based on Multisensor data fusion (AReM)', 'Dota2 Games Results', 'Facebook metrics', 'UbiqLog (smartphone lifelogging)', 'NIPS Conference Papers 1987-2015', 'HTRU2', 'Drug consumption (quantified)', 'Appliances energy prediction', 'Miskolc IIS Hybrid IPS', 'KDC-4007 dataset Collection', 'Geo-Magnetic field and WLAN dataset for indoor localisation from wristband and smartphone', 'DrivFace', 'Website Phishing', 'YouTube Spam Collection', 'Beijing PM2.5 Data', 'Cargo 2000 Freight Tracking and Tracing', 'Cervical cancer (Risk Factors)', 'Quality Assessment of Digital Colposcopies', 'KASANDR', 'FMA: A Dataset For Music Analysis', 'Air quality', 'Epileptic Seizure Recognition', 'Devanagari Handwritten Character Dataset', 'Stock portfolio performance', 'MoCap Hand Postures', 'Early biomarkers of Parkinson�s disease based on natural connected speech', 'Data for Software Engineering Teamwork Assessment in Education Setting', 'PM2.5 Data of Five Chinese Cities', 'Parkinson Disease Spiral Drawings Using Digitized Graphics Tablet', 'Sales_Transactions_Dataset_Weekly', 'Las Vegas Strip', 'Eco-hotel', 'MEU-Mobile KSD', 'Crowdsourced Mapping', 'gene expression cancer RNA-Seq', 'Hybrid Indoor Positioning Dataset from WiFi RSSI, Bluetooth and magnetometer', 'chestnut – LARVIC', 'Burst Header Packet (BHP) flooding attack on Optical Burst Switching (OBS) Network', 'Motion Capture Hand Postures', 'Anuran Calls (MFCCs)', 'TTC-3600: Benchmark dataset for Turkish text categorization', 'Gastrointestinal Lesions in Regular Colonoscopy', 'Daily Demand Forecasting Orders', 'Paper Reviews', 'extention of Z-Alizadeh sani dataset', 'Z-Alizadeh Sani', 'Dynamic Features of VirusShare Executables', 'IDA2016Challenge', 'DSRC Vehicle Communications', 'Mturk User-Perceived Clusters over Images', 'Character Font Images', 'DeliciousMIL: A Data Set for Multi-Label Multi-Instance Learning with Instance Labels', 'Autistic Spectrum Disorder Screening Data for Children', 'Autistic Spectrum Disorder Screening Data for Adolescent', 'APS Failure at Scania Trucks', 'Wireless Indoor Localization', 'HCC Survival', 'CSM (Conventional and Social Media Movies) Dataset 2014 and 2015', 'University of Tehran Question Dataset 2016 (UTQD.2016)', 'Autism Screening Adult', 'Activity recognition with healthy older people using a batteryless wearable sensor', 'Immunotherapy Dataset', 'Cryotherapy Dataset', 'OCT data & Color Fundus Images of Left & Right Eyes', 'Discrete Tone Image Dataset', 'News Popularity in Multiple Social Media Platforms', 'Ultrasonic flowmeter diagnostics', 'ICMLA 2014 Accepted Papers Data Set', 'BLE RSSI Dataset for Indoor localization and Navigation', 'Container Crane Controller Data Set', 'Residential Building Data Set', 'Health News in Twitter', 'chipseq', 'SGEMM GPU kernel performance', 'Repeat Consumption Matrices', 'detection_of_IoT_botnet_attacks_N_BaIoT', 'Absenteeism at work', 'SCADI', 'Condition monitoring of hydraulic systems', 'Carbon Nanotubes', 'Optical Interconnection Network', 'Sports articles for objectivity analysis', 'Breast Cancer Coimbra', 'GNFUV Unmanned Surface Vehicles Sensor Data', 'Dishonest Internet users Dataset', 'Victorian Era Authorship Attribution', 'Simulated Falls and Daily Living Activities Data Set', 'Multimodal Damage Identification for Humanitarian Computing', 'EEG Steady-State Visual Evoked Potential Signals', 'Roman Urdu Data Set', 'Avila', 'PANDOR', 'Drug Review Dataset (Druglib.com)', 'Drug Review Dataset (Drugs.com)', 'Physical Unclonable Functions', 'Superconductivty Data', 'WESAD (Wearable Stress and Affect Detection)', 'GNFUV Unmanned Surface Vehicles Sensor Data Set 2', 'Student Academics Performance', 'Online Shoppers Purchasing Intention Dataset', 'PMU-UD', \"Parkinson's Disease Classification\", 'Electrical Grid Stability Simulated Data', 'Caesarian Section Classification Dataset', 'BAUM-1', 'BAUM-2', 'Audit Data', 'BuddyMove Data Set', 'Real estate valuation data set', 'Early biomarkers of Parkinson’s disease based on natural connected speech Data Set', 'Somerville Happiness Survey', '2.4 GHZ Indoor Channel Measurements', 'EMG data for gestures', 'Parking Birmingham', 'Behavior of the urban traffic of the city of Sao Paulo in Brazil', 'Travel Reviews', 'Tarvel Review Ratings', 'Rice Leaf Diseases', 'Gas sensor array temperature modulation', 'Facebook Live Sellers in Thailand', 'Parkinson Dataset with replicated acoustic features', 'Metro Interstate Traffic Volume', 'Query Analytics Workloads Dataset', 'Wave Energy Converters', 'PPG-DaLiA', 'Alcohol QCM Sensor Dataset', 'Divorce Predictors data set', 'Incident management process enriched event log', 'Opinion Corpus for Lebanese Arabic Reviews (OCLAR)', 'MEx', 'Beijing Multi-Site Air-Quality Data', 'Online Retail II', 'Hepatitis C Virus (HCV) for Egyptian patients', 'QSAR fish toxicity', 'QSAR aquatic toxicity', 'Human Activity Recognition from Continuous Ambient Sensor Data', 'WISDM Smartphone and Smartwatch Activity and Biometrics Dataset', 'QSAR oral toxicity', 'QSAR androgen receptor', 'QSAR Bioconcentration classes dataset', 'QSAR fish bioconcentration factor (BCF)', 'A study of Asian Religious and Biblical Texts', 'Real-time Election Results: Portugal 2019', 'Bias correction of numerical prediction model temperature forecast', 'Bar Crawl: Detecting Heavy Drinking', 'Kitsune Network Attack Dataset', 'Shoulder Implant X-Ray Manufacturer Classification', 'Speaker Accent Recognition', 'Heart failure clinical records']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Dataset name\n",
    "\n",
    "Dataset_Name=[]\n",
    "\n",
    "dataset_name=driver.find_elements(By.XPATH,'//p[@class=\"normal\"]/b/a')\n",
    "for i in dataset_name[0:500]:\n",
    "    title=i.text\n",
    "    Dataset_Name.append(title)\n",
    "\n",
    "print(len(Dataset_Name),Dataset_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2b694fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ['Data Types', 'Multivariate ', 'Multivariate ', 'Multivariate ', ' ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Univariate, Text ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Data-Generator ', 'Multivariate ', 'Multivariate ', 'Domain-Theory ', 'Univariate, Time-Series ', 'Multivariate, Spatial ', 'Multivariate ', 'Multivariate, Domain-Theory ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Data-Generator ', ' ', ' ', 'Multivariate ', 'Multivariate ', 'Multivariate ', ' ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Relational ', 'Multivariate ', 'Multivariate, Data-Generator ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Domain-Theory ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Domain-Theory ', 'Sequential, Domain-Theory ', 'Sequential ', 'Sequential, Domain-Theory ', 'Multivariate ', 'Domain-Theory ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Domain-Theory ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Domain-Theory ', 'Domain-Theory ', 'Multivariate, Data-Generator ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', ' ', 'Domain-Theory ', 'Multivariate ', 'Multivariate ', 'Multivariate, Domain-Theory ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Data-Generator ', 'Multivariate, Data-Generator ', 'Multivariate ', 'Multivariate ', 'Multivariate ', ' ', 'Text ', 'Multivariate, Time-Series ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Relational ', 'Multivariate, Time-Series ', 'Spatio-temporal ', 'Transactional, Sequential ', 'Image ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Relational ', 'Multivariate, Relational ', 'Sequential ', 'Text ', 'Multivariate, Time-Series ', 'Univariate, Time-Series ', 'Text ', 'Multivariate, Time-Series ', 'Time-Series ', 'Multivariate, Text ', 'Text, Sequential ', 'Image ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', ' ', 'Domain-Theory ', ' ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential ', 'Multivariate ', 'Multivariate ', 'Text ', 'Text ', 'Multivariate ', 'Sequential ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Time-Series ', 'Multivariate ', 'Multivariate, Sequential ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential ', 'Multivariate, Time-Series ', 'Univariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Text ', 'Domain-Theory ', 'Multivariate, Sequential ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Time-Series ', 'Time-Series ', 'Multivariate, Text, Domain-Theory ', 'Time-Series, Domain-Theory ', 'Multivariate, Text, Domain-Theory ', 'Text ', 'Text ', 'Multivariate, Univariate, Text ', 'Multivariate, Univariate, Text ', 'Multivariate ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Univariate ', 'Multivariate, Text, Domain-Theory ', 'Univariate ', 'Univariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate, Text ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate, Univariate, Text ', 'Sequential ', 'Text ', 'Multivariate, Time-Series ', ' ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Sequential, Text ', 'Multivariate, Univariate, Time-Series ', 'Time-Series, Multivariate ', 'Multivariate ', 'Sequential ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Text ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Domain-Theory ', 'Multivariate, Text ', 'Multivariate, Time-Series ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series, Text ', 'Univariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate, Time-Series ', ' ', ' ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Univariate, Sequential, Time-Series ', 'Univariate, Sequential, Time-Series ', 'Multivariate ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Sequential ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Univariate, Domain-Theory ', 'Multivariate ', 'Multivariate, Time-Series ', ' ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate, Time-Series ', 'Multivariate ', 'Text ', 'Time-Series ', 'Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Time-Series ', 'Multivariate, Time-Series ', 'Multivariate, Univariate, Sequential, Text ', 'Multivariate ', 'Multivariate ', 'Multivariate ', ' ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Sequential, Time-Series ', 'Multivariate ', 'Multivariate, Sequential, Time-Series, Domain-Theory ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate, Time-Series ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate, Time-Series, Domain-Theory ', 'Multivariate, Time-Series ', 'Multivariate ', 'Time-Series ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Text ', 'Multivariate, Text ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Text ', 'Multivariate, Time-Series ', 'Multivariate, Sequential ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate, Time-Series ', 'Multivariate, Time-Series ', ' ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Sequential, Time-Series ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate, Time-Series ', ' ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', ' ', 'Text ', 'Multivariate ', 'Multivariate ', 'Text ', 'Multivariate ', 'Time-Series ', 'Text ', ' ', ' ', 'Multivariate, Time-Series ', 'Multivariate ', 'Sequential, Text ', 'Multivariate, Text ', 'Multivariate ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Text ', ' ', 'Sequential ', 'Univariate ', 'Univariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series, Text ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Univariate, Domain-Theory ', 'Multivariate ', 'Text ', 'Sequential ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate, Time-Series ', 'Univariate ', 'Multivariate ', 'Multivariate, Text ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Text ', 'Time-Series ', 'Multivariate, Text ', 'Multivariate, Time-Series ', 'Text ', 'Multivariate ', 'Multivariate ', 'Multivariate, Text ', 'Multivariate, Text ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Univariate ', 'Multivariate ', 'Multivariate ', 'Univariate ', 'Time-Series ', 'Time-Series ', 'Multivariate ', 'Multivariate, Text ', 'Multivariate ', 'Multivariate ', ' ', 'Multivariate ', 'Time-Series ', 'Multivariate, Univariate, Sequential, Time-Series ', 'Multivariate, Time-Series ', 'Multivariate, Text ', 'Multivariate, Text ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate, Univariate ', 'Multivariate, Sequential ', 'Text ', 'Time-Series ', 'Multivariate, Time-Series ', 'Multivariate, Sequential, Time-Series, Text ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Sequential, Time-Series ', 'Multivariate, Time-Series ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate ', 'Multivariate, Text ', 'Multivariate, Time-Series, Text ', 'Multivariate ', 'Multivariate, Time-Series ', 'Multivariate, Sequential, Time-Series ', 'Multivariate ', 'Multivariate ']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Data_Type \n",
    "\n",
    "Data_Type=[]\n",
    "\n",
    "datatype=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[2]\")\n",
    "for i in datatype[0:500]:\n",
    "    title=i.text\n",
    "    Data_Type.append(title)\n",
    "\n",
    "print(len(Data_Type),Data_Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1dcc8b29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ['Default Task', 'Classification ', 'Classification ', 'Classification ', 'Recommender-Systems ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Regression ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', ' ', ' ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Function-Learning ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Relational-Learning ', ' ', 'Classification ', 'Classification ', 'Classification ', ' ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', ' ', 'Classification ', 'Regression ', 'Classification ', 'Regression ', 'Classification ', 'Classification ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Clustering ', ' ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Clustering ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', ' ', 'Classification ', 'Classification ', 'Clustering ', 'Classification ', ' ', ' ', ' ', ' ', ' ', 'Recommender-Systems ', 'Classification ', 'Regression, Description ', ' ', ' ', 'Classification ', 'Regression ', 'Classification ', ' ', ' ', ' ', ' ', ' ', ' ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Classification ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', ' ', 'Classification ', 'Classification ', ' ', ' ', ' ', ' ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Regression ', 'Classification ', 'Clustering ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Causal-Discovery ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Causal-Discovery ', 'Clustering ', 'Classification, Clustering ', 'Regression ', 'Regression ', 'Classification ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Regression ', 'Classification ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Regression ', 'Classification ', ' ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Regression, Clustering, Causal-Discovery ', 'Classification, Clustering ', 'Classification ', 'Classification ', 'Classification, Regression, Clustering ', 'Classification, Regression, Clustering ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Classification ', 'Classification ', ' ', 'Classification ', 'Regression, Clustering ', 'Classification, Clustering ', 'Classification ', ' ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Classification, Regression ', 'Regression ', 'Classification, Regression ', 'Classification ', 'Regression, Clustering ', 'Classification, Regression ', 'Regression, Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Classification, Clustering ', 'Classification ', 'Clustering ', 'Classification, Clustering ', 'Classification ', 'Classification ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Classification, Regression, Clustering, Causa ', 'Classification, Clustering ', 'Regression ', 'Classification ', 'Regression ', 'Regression ', ' ', 'Classification ', ' ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Classification, Clustering ', 'Classification, Clustering ', 'Classification ', 'Classification, Clustering ', 'Classification, Regression, Clustering ', 'Regression ', 'Classification, Clustering ', 'Classification ', 'Regression ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Classification, Clustering, Causal-Discovery ', 'Classification, Regression, Clustering ', 'Classification, Regression ', 'Classification, Clustering ', 'Classification, Clustering ', 'Regression ', 'Classification ', 'Classification ', 'Clustering ', 'Classification, Regression ', 'Classification, Regression ', 'Classification, Regression ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Clustering ', 'Classification, Regression ', 'Regression ', 'Classification, Clustering ', 'Classification, Regression ', 'Classification ', 'Classification, Regression ', 'Regression, Clustering ', 'Classification, Regression ', 'Classification, Causal-Discovery ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Regression ', 'Classification ', 'Regression, Clustering, Causal-Discovery ', 'Regression ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Clustering, Causal-Discovery ', 'Classification, Regression ', 'Classification ', 'Classification, Clustering ', 'Classification, Regression, Clustering ', 'Classification, Clustering ', 'Classification, Regression, Clustering ', 'Classification ', 'Classification ', 'Classification, Regression, Clustering ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Causal-Discovery ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Classification, Regression, Clustering ', 'Classification, Clustering ', 'Regression ', 'Classification, Regression ', 'Classification ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Regression ', 'Causal-Discovery ', 'Clustering ', 'Classification, Clustering ', 'Classification ', 'Regression ', 'Classification, Clustering, Causal-Discovery ', 'Classification, Regression ', 'Classification, Regression, Clustering ', 'Classification, Regression, Clustering ', 'Classification ', 'Classification ', 'Regression ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Causal-Discovery ', 'Classification, Clustering ', 'Regression ', 'Classification, Clustering ', 'Classification ', 'Regression ', 'Classification, Clustering ', 'Classification, Regression ', 'Classification ', 'Regression ', 'Classification, Regression, Clustering ', 'Clustering ', 'Classification, Regression ', ' ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Classification, Clustering ', 'Classification, Clustering ', 'Classification, Clustering ', 'Classification ', 'Regression ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Classification, Regression ', 'Classification ', 'Clustering ', 'Clustering ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Regression ', 'Classification ', 'Classification, Clustering ', 'Classification, Clustering ', 'Classification, Regression ', 'Regression ', 'Clustering ', 'Classification ', 'Regression ', 'Clustering ', 'Classification, Clustering ', 'Classification, Clustering ', 'Classification, Clustering ', 'Classification, Regression ', 'Regression ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Regression ', 'Classification, Clustering ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Recommendation ', 'Classification, Regression, Clustering ', 'Classification, Regression, Clustering ', 'Classification ', 'Regression ', 'Classification, Regression ', 'Regression ', 'Classification ', 'Classification, Clustering ', 'Classification ', 'Classification ', 'Classification, Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Clustering ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Regression, Clustering ', 'Classification, Regression ', 'Classification, Clustering ', 'Classification, Clustering ', 'Classification ', 'Classification, Regression ', 'Clustering ', 'Classification ', 'Regression ', 'Regression, Clustering ', 'Regression ', 'Regression ', 'Classification, Regression, Clustering ', 'Classification ', 'Regression, Clustering ', 'Classification ', 'Classification, Clustering ', 'Regression ', 'Classification, Regression, Clustering ', 'Classification ', 'Regression ', 'Regression ', 'Classification ', 'Classification ', 'Classification ', 'Classification ', 'Classification, Regression ', 'Regression ', 'Classification, Clustering ', 'Regression ', 'Regression ', 'Classification, Regression ', 'Classification, Clustering, Causal-Discovery ', 'Classification ', 'Classification ']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Task \n",
    "\n",
    "Task=[]\n",
    "\n",
    "task_tags=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[3]\")\n",
    "for i in task_tags[0:500]:\n",
    "    title=i.text\n",
    "    Task.append(title)\n",
    "\n",
    "print(len(Task),Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8219da69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ['Attribute Types', 'Categorical, Integer, Real ', 'Categorical, Integer ', 'Categorical, Integer, Real ', 'Categorical ', 'Categorical, Integer, Real ', 'Categorical, Integer, Real ', 'Categorical ', 'Categorical ', 'Categorical, Real ', 'Categorical, Integer, Real ', ' ', 'Categorical ', 'Categorical ', 'Categorical ', 'Integer ', 'Real ', 'Real ', 'Categorical, Integer ', 'Categorical ', 'Categorical, Integer ', 'Categorical, Integer ', 'Categorical ', 'Categorical, Integer ', ' ', 'Categorical, Integer ', 'Categorical ', 'Categorical, Integer, Real ', 'Categorical, Real, Integer ', 'Integer ', 'Categorical, Integer ', 'Categorical, Integer ', 'Categorical, Integer, Real ', 'Categorical, Integer ', 'Categorical, Integer ', 'Real ', ' ', ' ', 'Categorical, Integer, Real ', 'Real ', 'Categorical, Integer ', 'Real ', 'Real ', 'Integer ', 'Categorical ', 'Categorical, Integer, Real ', 'Categorical, Integer, Real ', 'Categorical, Integer, Real ', 'Real ', 'Real ', 'Categorical, Integer, Real ', 'Integer, Real ', 'Real ', 'Real ', 'Categorical ', 'Categorical, Integer, Real ', 'Categorical ', 'Categorical ', 'Integer ', 'Categorical, Integer, Real ', ' ', 'Integer ', 'Categorical ', 'Categorical, Integer, Real ', 'Categorical, Integer, Real ', 'Categorical, Integer, Real ', 'Categorical ', 'Categorical ', 'Categorical ', 'Categorical ', ' ', 'Integer, Real ', 'Categorical ', 'Integer ', 'Integer ', 'Categorical ', ' ', 'Integer, Real ', 'Integer ', 'Integer ', 'Categorical, Integer ', 'Categorical ', ' ', ' ', 'Real ', 'Categorical, Integer ', 'Categorical ', 'Categorical ', 'Categorical ', 'Categorical ', 'Integer ', 'Integer, Real ', 'Integer, Real ', 'Categorical ', 'Integer ', 'Categorical, Integer ', ' ', ' ', 'Categorical, Integer ', 'Categorical ', 'Categorical, Real ', 'Categorical ', 'Categorical, Integer ', 'Categorical ', 'Integer, Real ', 'Real ', 'Real ', 'Integer, Real ', 'Real ', 'Categorical, Integer ', ' ', ' ', 'Categorical, Real ', 'Real ', 'Categorical ', 'Categorical, Integer ', 'Categorical, Real ', 'Real ', ' ', 'Categorical, Integer, Real ', 'Integer, Real ', 'Categorical ', 'Integer ', 'Categorical, Integer ', 'Categorical, Integer ', 'Categorical, Integer ', 'Real ', 'Categorical, Integer ', 'Categorical, Integer ', ' ', ' ', 'Categorical ', ' ', 'Categorical, Real ', ' ', 'Categorical ', 'Integer ', 'Real ', 'Categorical ', ' ', ' ', 'Categorical, Integer, Real ', 'Categorical, Integer ', 'Categorical, Real ', 'Integer ', 'Real ', 'Integer ', 'Integer ', 'Categorical ', 'Real ', 'Real ', ' ', ' ', 'Real ', 'Categorical, Integer ', 'Categorical, Integer ', 'Categorical, Integer ', 'Real ', 'Integer ', 'Integer ', 'Real ', ' ', 'Integer ', 'Real ', 'Real ', 'Real ', 'Integer ', 'Integer ', 'Integer ', 'Real ', 'Real ', 'Integer ', 'Real ', 'Real ', 'Real ', 'Integer ', 'Integer ', 'Real ', 'Categorical ', 'Real ', 'Real ', 'Real ', 'Categorical, Integer ', 'Real ', 'Integer, Real ', 'Real ', 'Integer, Real ', 'Integer ', ' ', 'Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Categorical, Integer, Real ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', ' ', 'Real ', 'Integer ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Real ', ' ', 'Real ', ' ', ' ', 'Integer, Real ', 'Integer, Real ', 'Real ', ' ', 'Real ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Real ', ' ', 'Integer ', 'Real ', 'Real ', 'Integer, Real ', 'Integer ', ' ', ' ', 'Real ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Integer, Real ', 'Real ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', 'Integer, Real ', ' ', 'Real ', 'Integer ', 'Real ', ' ', ' ', 'Real ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', 'Integer ', 'Integer, Real ', 'Real ', ' ', 'Integer, Real ', 'Real ', 'Real ', 'Integer, Real ', 'Integer, Real ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', ' ', 'Real ', ' ', ' ', 'Real ', 'Real ', 'Real ', ' ', 'Real ', 'Real ', 'Integer ', ' ', 'Real ', ' ', 'Integer ', ' ', 'Real ', 'Integer, Real ', 'Integer, Real ', 'Real ', 'Integer ', 'Integer, Real ', 'Real ', ' ', ' ', 'Real ', 'Real ', 'Integer, Real ', 'Integer ', 'Integer, Real ', 'Real ', ' ', 'Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Integer ', 'Real ', 'Real ', 'Integer, Real ', ' ', 'Real ', 'Real ', 'Integer ', 'Real ', 'Integer, Real ', 'Categorical ', ' ', 'Integer, Real ', ' ', ' ', 'Integer, Real ', 'Real ', 'Real ', ' ', 'Real ', 'Real ', 'Real ', 'Real ', 'Integer, Real ', 'Real ', 'Integer ', 'Real ', 'Real ', 'Integer ', 'Integer, Real ', 'Real ', 'Integer, Real ', 'Integer ', 'Real ', 'Real ', 'Real ', 'Real ', ' ', 'Real ', 'Real ', 'Real ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', ' ', 'Integer ', ' ', 'Integer ', 'Real ', 'Real ', 'Real ', 'Integer ', 'Integer ', 'Integer, Real ', 'Real ', 'Integer ', ' ', 'Integer, Real ', 'Integer ', 'Integer, Real ', 'Real ', 'Integer ', 'Real ', 'Real ', 'Integer, Real ', 'Integer ', 'Real ', 'Integer, Real ', 'Integer, Real ', 'Integer, Real ', 'Integer, Real ', 'Integer ', 'Integer, Real ', 'Integer ', ' ', 'Integer, Real ', ' ', 'Real ', 'Real ', ' ', 'Integer ', 'Real ', 'Real ', 'Integer ', 'Real ', 'Integer ', 'Integer ', 'Integer, Real ', 'Integer, Real ', 'Integer ', 'Integer ', 'Real ', 'Integer ', 'Integer, Real ', 'Integer ', 'Integer ', 'Integer ', 'Integer, Real ', 'Real ', 'Integer, Real ', 'Integer ', ' ', 'Integer ', 'Real ', 'Integer, Real ', 'Integer, Real ', 'Real ', ' ', 'Integer, Real ', 'Real ', ' ', 'Integer ', 'Real ', 'Real ', 'Real ', 'Integer ', 'Integer ', 'Real ', 'Real ', 'Integer, Real ', ' ', 'Real ', 'Real ', 'Integer, Real ', 'Integer ', 'Integer ', 'Real ', ' ', ' ', 'Integer ', 'Integer ', 'Integer ', ' ', 'Real ', 'Categorical ', 'Integer ', 'Integer ', 'Integer ', 'Real ', 'Real ', 'Real ', ' ', 'Integer, Real ', ' ', 'Integer, Real ', 'Real ', 'Integer ', ' ', ' ', 'Real ', 'Real ', 'Integer, Real ', 'Real ', 'Integer ', 'Real ', 'Real ', 'Real ', 'Integer, Real ', 'Real ', 'Real ', 'Integer ', 'Real ', 'Integer ', ' ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Integer ', 'Integer ', 'Integer ', 'Real ', 'Integer, Real ', 'Integer, Real ', 'Integer, Real ', 'Real ', 'Real ', 'Integer, Real ', 'Real ', ' ', ' ', ' ', 'Integer, Real ', 'Integer ', 'Integer, Real ', 'Real ', 'Real ', 'Real ', 'Real ', 'Real ']\n"
     ]
    }
   ],
   "source": [
    "#scraping the Attribute_Type \n",
    "\n",
    "Attribute_Type=[]\n",
    "\n",
    "ATT_tags=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[4]\")\n",
    "for i in ATT_tags[0:500]:\n",
    "    title=i.text\n",
    "    Attribute_Type.append(title)\n",
    "\n",
    "print(len(Attribute_Type),Attribute_Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2abb8530",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ['# Instances', '4177 ', '48842 ', '798 ', '37711 ', '452 ', '6000 ', '226 ', '226 ', '398 ', '205 ', '294 ', '625 ', '16 ', '286 ', '699 ', '198 ', '569 ', '108 ', '1728 ', '48842 ', ' ', '3196 ', '28056 ', ' ', '100 ', '67557 ', '690 ', '125 ', '209 ', '1473 ', '581012 ', '512 ', '366 ', ' ', ' ', ' ', ' ', '132 ', '336 ', '194 ', '352 ', '214 ', '306 ', '160 ', '303 ', '155 ', '368 ', ' ', '2310 ', '3279 ', '351 ', '150 ', '7797 ', '104 ', '57 ', ' ', '24 ', '20000 ', '345 ', ' ', '32 ', '148 ', '209 ', '528 ', ' ', '106 ', '128 ', '3190 ', '432 ', '202 ', '2000 ', '8124 ', '476 ', '6598 ', '12960 ', ' ', '5473 ', '5620 ', '10992 ', '90 ', '339 ', ' ', ' ', ' ', '167 ', '15 ', '1389 ', '307 ', '47 ', '23 ', '531 ', '4601 ', '267 ', '267 ', '76 ', ' ', '1000 ', '151 ', '958 ', '7200 ', '10 ', '285 ', '435 ', '527 ', '5000 ', '5000 ', '178 ', '1484 ', '101 ', ' ', '20000 ', '6650 ', '2565 ', '2458285 ', '299285 ', '340 ', '68040 ', ' ', '122 ', '178080 ', '50672 ', '640 ', '9000 ', '10104 ', '256932 ', '640 ', '191779 ', '4000000 ', ' ', '10000 ', '989818 ', '129000 ', ' ', '100000 ', '21578 ', '463 ', '600 ', '332 ', ' ', ' ', '690 ', '1000 ', '270 ', '6435 ', '2310 ', '58000 ', '946 ', '20008 ', '208 ', '528 ', ' ', ' ', '1024 ', '10080 ', '50400 ', '1025010 ', '19020 ', '1364 ', '961 ', '517 ', '200 ', '8000000 ', '1030 ', '606 ', '900 ', '2600 ', '1950 ', '13500 ', '4400 ', '2536 ', '300 ', '197 ', '2858 ', '748 ', '11640 ', '1593 ', '1567 ', '22632 ', '360 ', '103 ', '1994 ', '120 ', '4898 ', '2396130 ', '16772 ', '5875 ', '503 ', '51 ', '106 ', '2126 ', '5456 ', '8800 ', '164860 ', ' ', '1941 ', '130065 ', '515345 ', '440 ', ' ', '53500 ', '8235 ', ' ', '5749132 ', '2215 ', '310 ', '10000 ', '3000 ', '1500 ', '30000 ', '2500 ', '4143 ', '64 ', '53414 ', '65554 ', '45211 ', '1138562 ', '13910 ', '583 ', '2551 ', '34465 ', '5574 ', '245057 ', '182 ', '3850505 ', '138 ', '1080 ', '2075259 ', '210 ', '115 ', '3960456 ', ' ', '10299 ', '1600 ', '768 ', '308 ', '100 ', '237 ', '434874 ', '536 ', '140000 ', '6118 ', '165632 ', '18000 ', '540 ', '931 ', '1055 ', '100 ', '9120 ', '403 ', '111740 ', '10421 ', '5820 ', '403 ', '14980 ', '45730 ', '2584 ', '1372 ', '306 ', '120000 ', '13910 ', '2747 ', '3395 ', '39242 ', '4137 ', '17389 ', '51 ', '470 ', '132 ', '5000000 ', '11000000 ', '250 ', '126 ', ' ', '4889 ', ' ', ' ', '340 ', '501 ', '45781 ', '1503 ', '440 ', '2000 ', '9568 ', '168 ', '100000 ', '5665 ', '79 ', '127 ', '1040 ', '9900 ', '560 ', '60021 ', '1419 ', '101 ', '399 ', '58 ', '180 ', '21048 ', ' ', '750 ', '3000 ', '150 ', '1059 ', '11934 ', '27965 ', '216 ', '120 ', '649 ', '370 ', '4178504 ', '221579 ', '10800 ', '58509 ', '129685 ', '2456 ', '2921 ', '1151 ', '6590 ', '3000 ', '39797 ', '326 ', '913 ', '168286 ', '400 ', '314080 ', '637 ', '1710671 ', '12000 ', '10929 ', '1080 ', '40000 ', '43930257 ', '230318 ', '10500000 ', '13197 ', ' ', '30000 ', '324 ', '541909 ', '11164866 ', '163 ', '373 ', '20560 ', '40 ', '422937 ', '9358 ', '640 ', '919438 ', '40949 ', '5744 ', '10503 ', '42240 ', '102944 ', '500 ', '9782222 ', '11463 ', '17898 ', '1885 ', '19735 ', '1540 ', '4007 ', '153540 ', '606 ', '1353 ', '1956 ', '43824 ', '3942 ', '858 ', '287 ', '17764280 ', '106574 ', '9358 ', '11500 ', '92000 ', '315 ', '78095 ', '130 ', '74 ', '52854 ', '77 ', '811 ', '504 ', '401 ', '2856 ', '10546 ', '801 ', '1540 ', '1451 ', '1075 ', '78095 ', '7195 ', '3600 ', '76 ', '60 ', '405 ', '303 ', '303 ', '107888 ', '76000 ', '10000 ', '180 ', '745000 ', '12234 ', '292 ', '104 ', '60000 ', '2000 ', '165 ', '217 ', '1175 ', '704 ', '75128 ', '90 ', '90 ', '50 ', '71 ', '93239 ', '540 ', '105 ', '6611 ', '15 ', '372 ', '58000 ', '4960 ', '241600 ', '130000 ', '7062606 ', '740 ', '70 ', '2205 ', '10721 ', '640 ', '1000 ', '116 ', '1672 ', '322 ', '93600 ', '3060 ', '5879 ', '9200 ', '20000 ', '20867 ', ' ', '4143 ', '215063 ', '6000000 ', '21263 ', '63000000 ', '10190 ', '300 ', '12330 ', '5180 ', '756 ', '10000 ', '80 ', '1184 ', '1047 ', '777 ', '249 ', '414 ', ' ', '143 ', '7840 ', '30000 ', '35717 ', '135 ', '980 ', '5456 ', '120 ', '4095000 ', '7051 ', '240 ', '48204 ', '260000 ', '288000 ', '8300000 ', '125 ', '170 ', '141712 ', '3916 ', '6262 ', '420768 ', '1067371 ', '1385 ', '908 ', '546 ', '13956534 ', '15630426 ', '8992 ', '1687 ', '779 ', '1056 ', '590 ', '21643 ', '7750 ', '14057567 ', '27170754 ', '597 ', '329 ']\n"
     ]
    }
   ],
   "source": [
    "#scraping the No_of_Instances \n",
    "\n",
    "No_of_Instances=[]\n",
    "\n",
    "NOI_tags=driver.find_elements(By.XPATH,\"/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[5]\")\n",
    "for i in NOI_tags[0:500]:\n",
    "    title=i.text\n",
    "    No_of_Instances.append(title)\n",
    "\n",
    "print(len(No_of_Instances),No_of_Instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "74b48498",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ['# Attributes', '8 ', '14 ', '38 ', '294 ', '279 ', '7 ', ' ', '69 ', '8 ', '26 ', '1 ', '4 ', '4 ', '9 ', '10 ', '34 ', '32 ', '13 ', '6 ', '14 ', '22 ', '36 ', '6 ', ' ', '6 ', '42 ', '15 ', ' ', '9 ', '9 ', '54 ', '39 ', '33 ', '20 ', ' ', ' ', ' ', '12 ', '8 ', '30 ', ' ', '10 ', '3 ', '5 ', '75 ', '19 ', '27 ', ' ', '19 ', '1558 ', '34 ', '4 ', '617 ', '12 ', '16 ', '7 ', '4 ', '16 ', '7 ', ' ', '56 ', '18 ', '8 ', '22 ', ' ', '58 ', ' ', '61 ', '7 ', ' ', '649 ', '22 ', '168 ', '168 ', '8 ', ' ', '10 ', '64 ', '16 ', '8 ', '17 ', ' ', ' ', '72 ', '4 ', '6 ', '10 ', '35 ', '35 ', '4 ', '102 ', '57 ', '22 ', '44 ', '45 ', ' ', ' ', '5 ', '9 ', '21 ', '32 ', '17 ', '16 ', '38 ', '21 ', '40 ', '13 ', '8 ', '17 ', ' ', ' ', '15 ', '22 ', '68 ', '40 ', '17 ', '89 ', ' ', '4 ', '12 ', ' ', ' ', '86 ', '72 ', '61 ', '12 ', '481 ', '42 ', ' ', ' ', ' ', ' ', ' ', ' ', '5 ', '90 ', ' ', '5 ', ' ', ' ', '14 ', '20 ', '13 ', '36 ', '19 ', '9 ', '18 ', '4 ', '60 ', '10 ', ' ', ' ', '10 ', '4 ', '3 ', '11 ', '11 ', ' ', '6 ', '13 ', ' ', '100000 ', '9 ', '101 ', '10000 ', '20000 ', '100000 ', '5000 ', '500 ', '73 ', '43 ', '23 ', '3 ', '5 ', ' ', '256 ', '591 ', '70 ', '91 ', '10 ', '128 ', '6 ', '12 ', '3231961 ', '5409 ', '26 ', ' ', ' ', '10 ', '23 ', '24 ', '13 ', '8 ', ' ', '27 ', '50 ', '90 ', '138672 ', ' ', '386 ', ' ', ' ', '12 ', '147 ', '6 ', '8 ', '27 ', '10000 ', '20000 ', '10000 ', '54877 ', '4702 ', '24 ', '29 ', '17 ', '3 ', '128 ', '10 ', '242 ', '120 ', ' ', '4 ', '13 ', '52 ', '47 ', '857 ', '9 ', '7 ', '200 ', '4 ', ' ', '561 ', '64 ', '8 ', '7 ', '10 ', '9 ', '4 ', '8 ', '77 ', '51 ', '18 ', '1950000 ', '18 ', '1300 ', '41 ', '6 ', '5625 ', '5 ', ' ', '7 ', '33 ', '5 ', '15 ', '9 ', '19 ', '5 ', '5 ', '1000000 ', '129 ', ' ', '20 ', '152 ', '24 ', '16 ', '35 ', '17 ', '5 ', '18 ', '28 ', '7 ', '309 ', '3 ', '6 ', ' ', ' ', '16 ', '13 ', '5 ', '6 ', '8 ', '2 ', '4 ', '148 ', '55 ', '17 ', '8 ', '42 ', '26 ', '50 ', '2 ', '281 ', '120 ', ' ', '6 ', '120432 ', '150000 ', '529 ', ' ', '16 ', '2500 ', '5 ', '68 ', '16 ', '100 ', '216 ', '23 ', '33 ', '140256 ', '19 ', '20 ', '20 ', '49 ', '12 ', '30 ', '5232 ', '20 ', '1 ', ' ', '61 ', '27 ', '53 ', '11 ', '25 ', '0 ', '20 ', '9 ', '3 ', '561 ', '82 ', '13 ', '16 ', '13 ', '28 ', '4 ', ' ', '24 ', '34 ', '8 ', '128 ', '15 ', '513 ', '7 ', '7 ', '5 ', '15 ', '480000 ', '11 ', '54 ', '561 ', '64 ', '6 ', '116 ', '19 ', ' ', '5812 ', '9 ', '32 ', '29 ', '67 ', ' ', '25 ', '6400 ', '10 ', '5 ', '13 ', '98 ', '36 ', '69 ', '2158859 ', '518 ', '15 ', '179 ', ' ', '12 ', '38 ', '65 ', '102 ', '86 ', '7 ', '53 ', '20 ', '1 ', '71 ', '29 ', '20531 ', '65 ', '3 ', '22 ', '38 ', '22 ', '4814 ', '698 ', '13 ', '10 ', '59 ', '56 ', '482 ', '171 ', '5 ', '500 ', '411 ', '8519 ', '21 ', '21 ', '171 ', '7 ', '49 ', '12 ', '3 ', '21 ', '9 ', '8 ', '7 ', '2 ', '11 ', '11 ', '173 ', '5 ', '15 ', '3 ', '105 ', '25000 ', ' ', '18 ', '21000 ', '115 ', '21 ', '206 ', '43680 ', '8 ', '10 ', '59 ', '10 ', '5 ', '5 ', '1000 ', '138 ', ' ', '16 ', '2 ', '10 ', ' ', '8 ', '6 ', '129 ', '81 ', '12 ', '6 ', '22 ', '18 ', '9 ', '754 ', '14 ', '5 ', ' ', ' ', '18 ', '7 ', '7 ', ' ', '7 ', '5 ', '6 ', '4 ', '18 ', '11 ', '25 ', ' ', '20 ', '12 ', '46 ', '9 ', '8 ', '49 ', '11 ', '8 ', '54 ', '36 ', '3916 ', '710 ', '18 ', '8 ', '29 ', '7 ', '9 ', '37 ', '6 ', '1024 ', '1024 ', '14 ', '7 ', '8265 ', '29 ', '25 ', '3 ', '115 ', '1 ', '12 ']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the No of Attribute\n",
    "\n",
    "No_of_Attribute=[]\n",
    "\n",
    "NOA_tags=driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[6]')\n",
    "for i in NOA_tags[0:500]:\n",
    "    title=i.text\n",
    "    No_of_Attribute.append(title)\n",
    "\n",
    "print(len(No_of_Attribute),No_of_Attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c482994c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 ['Year', '1995 ', '1996 ', ' ', '1998 ', '1998 ', '1992 ', '1987 ', '1992 ', '1993 ', '1987 ', '1994 ', '1994 ', ' ', '1988 ', '1992 ', '1995 ', '1995 ', '1990 ', '1997 ', '1996 ', '1988 ', '1989 ', '1994 ', ' ', ' ', '1995 ', ' ', '1992 ', '1987 ', '1997 ', '1998 ', '1995 ', '1998 ', ' ', ' ', '1994 ', ' ', '1989 ', '1996 ', '1990 ', '1990 ', '1987 ', '1999 ', '1989 ', '1988 ', '1988 ', '1989 ', ' ', '1990 ', '1998 ', '1989 ', '1988 ', '1994 ', '1990 ', '1988 ', '1988 ', '1990 ', '1991 ', '1990 ', ' ', '1992 ', '1988 ', '1990 ', '1996 ', '1995 ', '1990 ', ' ', '1992 ', '1992 ', '1994 ', ' ', '1987 ', '1994 ', '1994 ', '1997 ', '1991 ', '1995 ', '1998 ', '1998 ', '1993 ', '1988 ', ' ', ' ', '1992 ', '1993 ', '1988 ', '1989 ', '1988 ', '1987 ', '1993 ', '1988 ', '1999 ', '2001 ', '2001 ', ' ', '1992 ', '1993 ', '1997 ', '1991 ', '1987 ', '1994 ', '1988 ', '1987 ', '1993 ', '1988 ', '1988 ', '1991 ', '1996 ', '1990 ', ' ', '1999 ', '1999 ', '2002 ', ' ', '2000 ', '1999 ', '1999 ', '2001 ', '1999 ', '1999 ', '2000 ', '1999 ', '2000 ', '1999 ', '1999 ', ' ', '1998 ', '1999 ', '2001 ', '1999 ', ' ', '2003 ', '1999 ', '1999 ', '1997 ', '1999 ', '1999 ', '1998 ', ' ', ' ', ' ', '1994 ', ' ', '1993 ', '1990 ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', '1989 ', '2006 ', '2006 ', '2007 ', '2007 ', '2007 ', '2007 ', '2008 ', '2008 ', '2008 ', '2007 ', '2008 ', '2008 ', '2008 ', '2008 ', '2008 ', '2008 ', '2008 ', '2008 ', '2008 ', '2008 ', '2008 ', '2009 ', '2008 ', '2008 ', '2008 ', '2009 ', '2009 ', '2009 ', '2009 ', '2009 ', '2009 ', '2010 ', '2009 ', '2010 ', '2010 ', '2010 ', '2010 ', '2010 ', '2010 ', '2010 ', '2010 ', '2010 ', '2010 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2011 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2012 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2013 ', '2014 ', '2013 ', '2013 ', '2013 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2013 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2014 ', '2015 ', '2014 ', '2014 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2014 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2015 ', '2016 ', '2016 ', '2015 ', '2016 ', '2016 ', '2015 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2016 ', '2017 ', '2016 ', '2017 ', '2016 ', '2017 ', '2017 ', '2016 ', '2016 ', '2017 ', '2017 ', '2016 ', '2017 ', '2017 ', '2017 ', '2017 ', '2016 ', '2017 ', '2016 ', '2016 ', '2016 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2016 ', '2016 ', '2016 ', '2016 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2016 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2016 ', '2016 ', '2016 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2017 ', '2016 ', '2018 ', '2018 ', '2016 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2018 ', '2019 ', '2019 ', '2018 ', '2018 ', '2018 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2019 ', '2020 ', '2020 ', '2019 ', '2020 ', '2020 ']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the year\n",
    "\n",
    "Year=[]\n",
    "\n",
    "year_tags=driver.find_elements(By.XPATH,'/html/body/table[2]/tbody/tr/td[2]/table[2]/tbody/tr/td[7]/p')\n",
    "for i in year_tags[0:500]:\n",
    "    title=i.text\n",
    "    Year.append(title)\n",
    "\n",
    "print(len(Year),Year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a3482d12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset_Name</th>\n",
       "      <th>Data_Type</th>\n",
       "      <th>Task</th>\n",
       "      <th>Attribute_Type</th>\n",
       "      <th>No_of_Instances</th>\n",
       "      <th>No_of_Attribute</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abalone</td>\n",
       "      <td>Data Types</td>\n",
       "      <td>Default Task</td>\n",
       "      <td>Attribute Types</td>\n",
       "      <td># Instances</td>\n",
       "      <td># Attributes</td>\n",
       "      <td>Year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Adult</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>4177</td>\n",
       "      <td>8</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Annealing</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer</td>\n",
       "      <td>48842</td>\n",
       "      <td>14</td>\n",
       "      <td>1996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anonymous Microsoft Web Data</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Categorical, Integer, Real</td>\n",
       "      <td>798</td>\n",
       "      <td>38</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Arrhythmia</td>\n",
       "      <td></td>\n",
       "      <td>Recommender-Systems</td>\n",
       "      <td>Categorical</td>\n",
       "      <td>37711</td>\n",
       "      <td>294</td>\n",
       "      <td>1998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>Bar Crawl: Detecting Heavy Drinking</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>7750</td>\n",
       "      <td>25</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>Kitsune Network Attack Dataset</td>\n",
       "      <td>Multivariate, Time-Series</td>\n",
       "      <td>Classification, Regression</td>\n",
       "      <td>Real</td>\n",
       "      <td>14057567</td>\n",
       "      <td>3</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>Shoulder Implant X-Ray Manufacturer Classifica...</td>\n",
       "      <td>Multivariate, Sequential, Time-Series</td>\n",
       "      <td>Classification, Clustering, Causal-Discovery</td>\n",
       "      <td>Real</td>\n",
       "      <td>27170754</td>\n",
       "      <td>115</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>Speaker Accent Recognition</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>597</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>Heart failure clinical records</td>\n",
       "      <td>Multivariate</td>\n",
       "      <td>Classification</td>\n",
       "      <td>Real</td>\n",
       "      <td>329</td>\n",
       "      <td>12</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Dataset_Name  \\\n",
       "0                                              Abalone   \n",
       "1                                                Adult   \n",
       "2                                            Annealing   \n",
       "3                         Anonymous Microsoft Web Data   \n",
       "4                                           Arrhythmia   \n",
       "..                                                 ...   \n",
       "495                Bar Crawl: Detecting Heavy Drinking   \n",
       "496                     Kitsune Network Attack Dataset   \n",
       "497  Shoulder Implant X-Ray Manufacturer Classifica...   \n",
       "498                         Speaker Accent Recognition   \n",
       "499                     Heart failure clinical records   \n",
       "\n",
       "                                  Data_Type  \\\n",
       "0                                Data Types   \n",
       "1                             Multivariate    \n",
       "2                             Multivariate    \n",
       "3                             Multivariate    \n",
       "4                                             \n",
       "..                                      ...   \n",
       "495                           Multivariate    \n",
       "496              Multivariate, Time-Series    \n",
       "497  Multivariate, Sequential, Time-Series    \n",
       "498                           Multivariate    \n",
       "499                           Multivariate    \n",
       "\n",
       "                                              Task  \\\n",
       "0                                     Default Task   \n",
       "1                                  Classification    \n",
       "2                                  Classification    \n",
       "3                                  Classification    \n",
       "4                             Recommender-Systems    \n",
       "..                                             ...   \n",
       "495                                    Regression    \n",
       "496                    Classification, Regression    \n",
       "497  Classification, Clustering, Causal-Discovery    \n",
       "498                                Classification    \n",
       "499                                Classification    \n",
       "\n",
       "                  Attribute_Type No_of_Instances No_of_Attribute   Year  \n",
       "0                Attribute Types     # Instances    # Attributes   Year  \n",
       "1    Categorical, Integer, Real            4177               8   1995   \n",
       "2          Categorical, Integer           48842              14   1996   \n",
       "3    Categorical, Integer, Real             798              38          \n",
       "4                   Categorical           37711             294   1998   \n",
       "..                           ...             ...             ...    ...  \n",
       "495                        Real            7750              25   2020   \n",
       "496                        Real        14057567               3   2020   \n",
       "497                        Real        27170754             115   2019   \n",
       "498                        Real             597               1   2020   \n",
       "499                        Real             329              12   2020   \n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "Recruiters=pd.DataFrame({'Dataset_Name':Dataset_Name,'Data_Type': Data_Type,'Task':Task,'Attribute_Type':Attribute_Type,'No_of_Instances':No_of_Instances,'No_of_Attribute':No_of_Attribute,'Year':Year})\n",
    "Recruiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f155c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f206f89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ce91cc3",
   "metadata": {},
   "source": [
    "<a id=\"naukri\"> </a>\n",
    "## 9. Scrape the details of Data science recruiters Url = https://www.naukri.com/hr-recruiters-consultants.\n",
    "\n",
    "You have to find the following details: \n",
    "\n",
    "    A) Name\n",
    "    B) Designation\n",
    "    C)Company \n",
    "    D)Skills they hire for \n",
    "    E) Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "aa9af4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's first connect to the driver\n",
    "\n",
    "driver=webdriver.Chrome(r\"C:\\Users\\HP\\Internship\\chromedriver_win32\\chromedriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5e54fbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening the Naukri page on automated chrome browser\n",
    "\n",
    "driver.get('https://www.naukri.com/hr-recruiters-consultants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "72016750",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entering designation as required in the question\n",
    "\n",
    "designation=driver.find_element(By.CLASS_NAME,\"sugInp\")\n",
    "designation.send_keys(\"Data science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4e8c4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search\n",
    "\n",
    "search=driver.find_element(By.XPATH,\"/html/body/div[2]/div[2]/div[1]/div[1]/form/div[1]/button\")\n",
    "search.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ab76aa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['Aakash Harit', 'shravan Kumar Gaddam', 'MARSIAN Technologies LLP', 'Anik Agrawal', 'subhas patel', 'Abhishek - Only Analytics Hiring - India and', 'Institute for Financial Management and Resear', 'Balu Ramesh', 'Asif Lucknowi', 'InstaFinancials', 'Kalpana Dumpala', 'Mubarak', 'Kushal Rastogi', 'Priyanka Akiri', 'Kapil Devang', 'Mahesh Babu Channa', 'Vaishnavi Kudalkar', 'Sakshi Chhikara', 'Ruchi Dhote', 'Manisha Yadav', 'Riya Rajesh', 'Rashmi Bhattacharjee', 'Faizan Kareem', 'Rithika dadwal', 'Sandhya Khandagale']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Recruiters name\n",
    "\n",
    "Name=[]\n",
    "\n",
    "name_tags=driver.find_elements(By.XPATH,'//span[@class=\"fl ellipsis\"]')\n",
    "for i in name_tags[0:25]:\n",
    "    title=i.text\n",
    "    Name.append(title)\n",
    "\n",
    "print(len(Name),Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dcde02b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['HR Manager', 'Company Recruiter', 'Company HR', 'Company Recruiter', 'Founder CEO', 'Recruitment Lead Consultant', 'Programme Manager', 'HR Administrator', 'Director', 'Human Resource', 'Executive Hiring', 'Company HR', 'Company HR', 'HR Manager', 'HR Manager', 'HR Team Lead', 'HR Executive', 'Assistant Manager HR', 'Senior Executive Talent Acquisition', 'HR Executive', 'Manager Talent Acquisition', 'HR Head', 'HR MANAGER', 'HR Recruiter', 'HR Recruiter']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the Designation\n",
    "\n",
    "Designation=[]\n",
    "\n",
    "designation_tags=driver.find_elements(By.XPATH,'//span[@class=\"ellipsis clr\"]')\n",
    "for i in designation_tags[0:25]:\n",
    "    title=i.text\n",
    "    Designation.append(title)\n",
    "\n",
    "print(len(Designation),Designation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1663b880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['Data Science Network', 'Shore Infotech India Pvt. Ltd', 'MARSIAN Technologies LLP', 'Enerlytics Software Solutions Pvt Ltd', 'LibraryXProject', 'Apidel Technologies Division of Transpower', 'IFMR', 'Techvantage Systems Pvt Ltd', 'Weupskill- Live Wire India', 'CBL Data Science Private Limited', 'Innominds Software', 'MoneyTap', 'QuantMagnum Technologies Pvt. Ltd.', 'Infinitive Software Solutions', 'BISP Solutions', 'SocialPrachar.com', 'Codeachive learning', 'BIZ INFOTECNO PRIVATE LIMITED', 'Bristlecone India Ltd', 'Easi Tax', 'Novelworx Digital Solutions', 'AXESTRACK SOFTWARE SOLUTIONS PRIVATE...', 'FirstTech Consaltants Pvt.Ltd', 'Affine Analytics', 'Compumatrice Multimedia Pvt Ltd']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the company name\n",
    "\n",
    "Company=[]\n",
    "\n",
    "company_tags=driver.find_elements(By.XPATH,'//p[@class=\"highlightable\"]/a[2]')\n",
    "for i in company_tags[0:25]:\n",
    "    title=i.text\n",
    "    Company.append(title)\n",
    "\n",
    "print(len(Company),Company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e5332893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['Classic ASP Developer, Internet Marketing Professional, Data Science SME, Content Writers, SEO Professional, Revenue Professional', '.Net, Java, Data Science, Linux Administration, Sql Server Development, Winforms, Wcf Services, Wpf, Telecom Engineering, Technical Management, Software', 'Data Science, Artificial Intelligence, Machine Learning, Business Analytics, Deep Learning, statistics, Data Analytics, Data Analysis, support vector machine', 'Mean Stack, javascript, angularjs, mongodb, Web Services, rest, express, Node.js, Big Data, iot, Data Science, Cloud Computing, saas, Aws', 'Hadoop, Spark, Digital Strategy, Data Architecture, Command Center, Cdp, Dmp, Kafka, Data Science, Data Analysis, Big Data Analytics, Real Time Analysis, SQL', 'Analytics, Business Intelligence, Business Analytics, Predictive Modeling, Predictive Analytics, Data Science, Data Analysis, Data Analytics, Big Data, Big', 'Data Science', 'Machine Learning, algorithms, Go Getter, Computer Science, spark, Big Data, hdfs, sql, cassandra, hadoop, python, scala, java, Data Science, Front End', 'Technical Training, Software Development, Presentation Skills, B.tech, M.tech, B.e., mca, msc, Computer Science, freshers, jobs in indore, Data Science, itil', 'Software Development, It Sales, Account Management, Data Analysis, Customer Service, Sr, Software Engineering, Mvc, Ajax, Asp.net, Html, C#, Javascript', 'Qa, Ui/ux, Java Developer, Java Architect, C++/qt, Php, Lamp, Api, J2ee, Java, Soa, Esb, Middleware, Bigdata Achitect, Hadoop Architect, Deep', 'Business Intelligence, Data Warehousing, Data Science, Business Analytics, Customer Support, Business Reporting, Bi', 'Office Administration, Hr Administration, telecalling, client relationship management, Client Acquisition, Sales, Reception, HR, Recruitment, Onboarding, Human', 'Oracle Dba, Data Science, Data Warehousing, ETL, Jupyter, Numpy, Data Transformation, Snowflake, Teradata, Python, Data Manipulation, Relational Databases', 'Big Data, Hadoop, Data Analytics, Data Science', 'Social Media, digital media maketing, seo, smm, smo, sem, Content Wirting, social media marketing, social media manager, digital media marketing manager', 'Data Science, Python, Data Analytics', 'React.js, Data Science, Java, Front End, Business Analytics, Backend, Tableau, Python, Qa Testing, Automation Testing', 'Qlikview, Qlik Sense, Microsoft Azure, Power Bi, Data Science, Machine Learning', 'Telecalling, Client Interaction, Marketing, Research, Web Development, Social Media Marketing, Data Entry Operation, Excel, Ms Office, Invoicing', 'Data Science', 'Corporate Sales, Software Development, Software Sales, Marketing, Creative Designing, Corporate Planning, Senior Management, Crm, Client Relationship', 'Data Analytics, Data Science, Machine Learning, Deep Learning, Nlp, Data Mining, Python, R, Database Administration, Text Mining', 'Data Science, Machine Learning, Python, R, Deep Learning, Big Data, Hadoop', 'Big Data, Data Science, Artificial Intelligence, Hadoop, Ui Development, Php, Freelancing, .Net, Software Testing, Sap, Leadership Hiring']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the skills recruiters hire for\n",
    "\n",
    "Skills=[]\n",
    "\n",
    "skillshire_tags=driver.find_elements(By.XPATH,'//div[@class=\"hireSec highlightable\"]')\n",
    "for i in skillshire_tags[0:25]:\n",
    "    title=i.text\n",
    "    Skills.append(title)\n",
    "\n",
    "print(len(Skills),Skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "118ee688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 ['Delhi', 'Hyderabad / Secunderabad', 'Pune', 'Ahmedabad', 'UK - (london)', 'Vadodara / Baroda', 'Chennai', 'Trivandrum', 'Indore', 'Bengaluru / Bangalore', 'Hyderabad / Secunderabad', 'Bengaluru / Bangalore', 'Mumbai', 'Hyderabad', 'Bhopal', 'Hyderabad / Secunderabad', 'Mumbai', 'Chandigarh', 'Pune', 'Navi Mumbai', 'Cochin', 'Delhi', 'Hyderabad / Secunderabad', 'Pune', 'Pune']\n"
     ]
    }
   ],
   "source": [
    "#Scrap the locations\n",
    "\n",
    "Locations=[]\n",
    "\n",
    "location_tags=driver.find_elements(By.XPATH,'//small[@class=\"ellipsis\"]')\n",
    "for i in location_tags[0:25]:\n",
    "    title=i.text\n",
    "    Locations.append(title)\n",
    "\n",
    "print(len(Locations),Locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c1196ee9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Designation</th>\n",
       "      <th>Company</th>\n",
       "      <th>Skills they hire for</th>\n",
       "      <th>Locations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aakash Harit</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Data Science Network</td>\n",
       "      <td>Classic ASP Developer, Internet Marketing Prof...</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shravan Kumar Gaddam</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>Shore Infotech India Pvt. Ltd</td>\n",
       "      <td>.Net, Java, Data Science, Linux Administration...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>MARSIAN Technologies LLP</td>\n",
       "      <td>Data Science, Artificial Intelligence, Machine...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anik Agrawal</td>\n",
       "      <td>Company Recruiter</td>\n",
       "      <td>Enerlytics Software Solutions Pvt Ltd</td>\n",
       "      <td>Mean Stack, javascript, angularjs, mongodb, We...</td>\n",
       "      <td>Ahmedabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>subhas patel</td>\n",
       "      <td>Founder CEO</td>\n",
       "      <td>LibraryXProject</td>\n",
       "      <td>Hadoop, Spark, Digital Strategy, Data Architec...</td>\n",
       "      <td>UK - (london)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Abhishek - Only Analytics Hiring - India and</td>\n",
       "      <td>Recruitment Lead Consultant</td>\n",
       "      <td>Apidel Technologies Division of Transpower</td>\n",
       "      <td>Analytics, Business Intelligence, Business Ana...</td>\n",
       "      <td>Vadodara / Baroda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Institute for Financial Management and Resear</td>\n",
       "      <td>Programme Manager</td>\n",
       "      <td>IFMR</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Chennai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Balu Ramesh</td>\n",
       "      <td>HR Administrator</td>\n",
       "      <td>Techvantage Systems Pvt Ltd</td>\n",
       "      <td>Machine Learning, algorithms, Go Getter, Compu...</td>\n",
       "      <td>Trivandrum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Asif Lucknowi</td>\n",
       "      <td>Director</td>\n",
       "      <td>Weupskill- Live Wire India</td>\n",
       "      <td>Technical Training, Software Development, Pres...</td>\n",
       "      <td>Indore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>InstaFinancials</td>\n",
       "      <td>Human Resource</td>\n",
       "      <td>CBL Data Science Private Limited</td>\n",
       "      <td>Software Development, It Sales, Account Manage...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Kalpana Dumpala</td>\n",
       "      <td>Executive Hiring</td>\n",
       "      <td>Innominds Software</td>\n",
       "      <td>Qa, Ui/ux, Java Developer, Java Architect, C++...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Mubarak</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>MoneyTap</td>\n",
       "      <td>Business Intelligence, Data Warehousing, Data ...</td>\n",
       "      <td>Bengaluru / Bangalore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kushal Rastogi</td>\n",
       "      <td>Company HR</td>\n",
       "      <td>QuantMagnum Technologies Pvt. Ltd.</td>\n",
       "      <td>Office Administration, Hr Administration, tele...</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Priyanka Akiri</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>Infinitive Software Solutions</td>\n",
       "      <td>Oracle Dba, Data Science, Data Warehousing, ET...</td>\n",
       "      <td>Hyderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Kapil Devang</td>\n",
       "      <td>HR Manager</td>\n",
       "      <td>BISP Solutions</td>\n",
       "      <td>Big Data, Hadoop, Data Analytics, Data Science</td>\n",
       "      <td>Bhopal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Mahesh Babu Channa</td>\n",
       "      <td>HR Team Lead</td>\n",
       "      <td>SocialPrachar.com</td>\n",
       "      <td>Social Media, digital media maketing, seo, smm...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Vaishnavi Kudalkar</td>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Codeachive learning</td>\n",
       "      <td>Data Science, Python, Data Analytics</td>\n",
       "      <td>Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Sakshi Chhikara</td>\n",
       "      <td>Assistant Manager HR</td>\n",
       "      <td>BIZ INFOTECNO PRIVATE LIMITED</td>\n",
       "      <td>React.js, Data Science, Java, Front End, Busin...</td>\n",
       "      <td>Chandigarh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ruchi Dhote</td>\n",
       "      <td>Senior Executive Talent Acquisition</td>\n",
       "      <td>Bristlecone India Ltd</td>\n",
       "      <td>Qlikview, Qlik Sense, Microsoft Azure, Power B...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Manisha Yadav</td>\n",
       "      <td>HR Executive</td>\n",
       "      <td>Easi Tax</td>\n",
       "      <td>Telecalling, Client Interaction, Marketing, Re...</td>\n",
       "      <td>Navi Mumbai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Riya Rajesh</td>\n",
       "      <td>Manager Talent Acquisition</td>\n",
       "      <td>Novelworx Digital Solutions</td>\n",
       "      <td>Data Science</td>\n",
       "      <td>Cochin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Rashmi Bhattacharjee</td>\n",
       "      <td>HR Head</td>\n",
       "      <td>AXESTRACK SOFTWARE SOLUTIONS PRIVATE...</td>\n",
       "      <td>Corporate Sales, Software Development, Softwar...</td>\n",
       "      <td>Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Faizan Kareem</td>\n",
       "      <td>HR MANAGER</td>\n",
       "      <td>FirstTech Consaltants Pvt.Ltd</td>\n",
       "      <td>Data Analytics, Data Science, Machine Learning...</td>\n",
       "      <td>Hyderabad / Secunderabad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Rithika dadwal</td>\n",
       "      <td>HR Recruiter</td>\n",
       "      <td>Affine Analytics</td>\n",
       "      <td>Data Science, Machine Learning, Python, R, Dee...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Sandhya Khandagale</td>\n",
       "      <td>HR Recruiter</td>\n",
       "      <td>Compumatrice Multimedia Pvt Ltd</td>\n",
       "      <td>Big Data, Data Science, Artificial Intelligenc...</td>\n",
       "      <td>Pune</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Name  \\\n",
       "0                                    Aakash Harit   \n",
       "1                            shravan Kumar Gaddam   \n",
       "2                        MARSIAN Technologies LLP   \n",
       "3                                    Anik Agrawal   \n",
       "4                                    subhas patel   \n",
       "5    Abhishek - Only Analytics Hiring - India and   \n",
       "6   Institute for Financial Management and Resear   \n",
       "7                                     Balu Ramesh   \n",
       "8                                   Asif Lucknowi   \n",
       "9                                 InstaFinancials   \n",
       "10                                Kalpana Dumpala   \n",
       "11                                        Mubarak   \n",
       "12                                 Kushal Rastogi   \n",
       "13                                 Priyanka Akiri   \n",
       "14                                   Kapil Devang   \n",
       "15                             Mahesh Babu Channa   \n",
       "16                             Vaishnavi Kudalkar   \n",
       "17                                Sakshi Chhikara   \n",
       "18                                    Ruchi Dhote   \n",
       "19                                  Manisha Yadav   \n",
       "20                                    Riya Rajesh   \n",
       "21                           Rashmi Bhattacharjee   \n",
       "22                                  Faizan Kareem   \n",
       "23                                 Rithika dadwal   \n",
       "24                             Sandhya Khandagale   \n",
       "\n",
       "                            Designation  \\\n",
       "0                            HR Manager   \n",
       "1                     Company Recruiter   \n",
       "2                            Company HR   \n",
       "3                     Company Recruiter   \n",
       "4                           Founder CEO   \n",
       "5           Recruitment Lead Consultant   \n",
       "6                     Programme Manager   \n",
       "7                      HR Administrator   \n",
       "8                              Director   \n",
       "9                        Human Resource   \n",
       "10                     Executive Hiring   \n",
       "11                           Company HR   \n",
       "12                           Company HR   \n",
       "13                           HR Manager   \n",
       "14                           HR Manager   \n",
       "15                         HR Team Lead   \n",
       "16                         HR Executive   \n",
       "17                 Assistant Manager HR   \n",
       "18  Senior Executive Talent Acquisition   \n",
       "19                         HR Executive   \n",
       "20           Manager Talent Acquisition   \n",
       "21                              HR Head   \n",
       "22                           HR MANAGER   \n",
       "23                         HR Recruiter   \n",
       "24                         HR Recruiter   \n",
       "\n",
       "                                       Company  \\\n",
       "0                         Data Science Network   \n",
       "1                Shore Infotech India Pvt. Ltd   \n",
       "2                     MARSIAN Technologies LLP   \n",
       "3        Enerlytics Software Solutions Pvt Ltd   \n",
       "4                              LibraryXProject   \n",
       "5   Apidel Technologies Division of Transpower   \n",
       "6                                         IFMR   \n",
       "7                  Techvantage Systems Pvt Ltd   \n",
       "8                   Weupskill- Live Wire India   \n",
       "9             CBL Data Science Private Limited   \n",
       "10                          Innominds Software   \n",
       "11                                    MoneyTap   \n",
       "12          QuantMagnum Technologies Pvt. Ltd.   \n",
       "13               Infinitive Software Solutions   \n",
       "14                              BISP Solutions   \n",
       "15                           SocialPrachar.com   \n",
       "16                         Codeachive learning   \n",
       "17               BIZ INFOTECNO PRIVATE LIMITED   \n",
       "18                       Bristlecone India Ltd   \n",
       "19                                    Easi Tax   \n",
       "20                 Novelworx Digital Solutions   \n",
       "21     AXESTRACK SOFTWARE SOLUTIONS PRIVATE...   \n",
       "22               FirstTech Consaltants Pvt.Ltd   \n",
       "23                            Affine Analytics   \n",
       "24             Compumatrice Multimedia Pvt Ltd   \n",
       "\n",
       "                                 Skills they hire for  \\\n",
       "0   Classic ASP Developer, Internet Marketing Prof...   \n",
       "1   .Net, Java, Data Science, Linux Administration...   \n",
       "2   Data Science, Artificial Intelligence, Machine...   \n",
       "3   Mean Stack, javascript, angularjs, mongodb, We...   \n",
       "4   Hadoop, Spark, Digital Strategy, Data Architec...   \n",
       "5   Analytics, Business Intelligence, Business Ana...   \n",
       "6                                        Data Science   \n",
       "7   Machine Learning, algorithms, Go Getter, Compu...   \n",
       "8   Technical Training, Software Development, Pres...   \n",
       "9   Software Development, It Sales, Account Manage...   \n",
       "10  Qa, Ui/ux, Java Developer, Java Architect, C++...   \n",
       "11  Business Intelligence, Data Warehousing, Data ...   \n",
       "12  Office Administration, Hr Administration, tele...   \n",
       "13  Oracle Dba, Data Science, Data Warehousing, ET...   \n",
       "14     Big Data, Hadoop, Data Analytics, Data Science   \n",
       "15  Social Media, digital media maketing, seo, smm...   \n",
       "16               Data Science, Python, Data Analytics   \n",
       "17  React.js, Data Science, Java, Front End, Busin...   \n",
       "18  Qlikview, Qlik Sense, Microsoft Azure, Power B...   \n",
       "19  Telecalling, Client Interaction, Marketing, Re...   \n",
       "20                                       Data Science   \n",
       "21  Corporate Sales, Software Development, Softwar...   \n",
       "22  Data Analytics, Data Science, Machine Learning...   \n",
       "23  Data Science, Machine Learning, Python, R, Dee...   \n",
       "24  Big Data, Data Science, Artificial Intelligenc...   \n",
       "\n",
       "                   Locations  \n",
       "0                      Delhi  \n",
       "1   Hyderabad / Secunderabad  \n",
       "2                       Pune  \n",
       "3                  Ahmedabad  \n",
       "4              UK - (london)  \n",
       "5          Vadodara / Baroda  \n",
       "6                    Chennai  \n",
       "7                 Trivandrum  \n",
       "8                     Indore  \n",
       "9      Bengaluru / Bangalore  \n",
       "10  Hyderabad / Secunderabad  \n",
       "11     Bengaluru / Bangalore  \n",
       "12                    Mumbai  \n",
       "13                 Hyderabad  \n",
       "14                    Bhopal  \n",
       "15  Hyderabad / Secunderabad  \n",
       "16                    Mumbai  \n",
       "17                Chandigarh  \n",
       "18                      Pune  \n",
       "19               Navi Mumbai  \n",
       "20                    Cochin  \n",
       "21                     Delhi  \n",
       "22  Hyderabad / Secunderabad  \n",
       "23                      Pune  \n",
       "24                      Pune  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create DataFrame\n",
    "\n",
    "import pandas as pd\n",
    "Recruiters=pd.DataFrame({'Name':Name,'Designation': Designation,'Company':Company,'Skills they hire for':Skills,'Locations':Locations})\n",
    "Recruiters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e760a02e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc580121",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca800b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
